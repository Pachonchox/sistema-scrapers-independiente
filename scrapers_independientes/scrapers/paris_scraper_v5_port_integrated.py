# -*- coding: utf-8 -*-
"""
ğŸ¬ Paris Chile Scraper v5 - INTEGRACIÃ“N COMPLETA PORT
====================================================

ESTE SCRAPER INTEGRA COMPLETAMENTE LA LÃ“GICA FUNCIONAL DEL scrappers_port/paris_final.py
QUE SÃ EXTRAE PRODUCTOS, ADAPTÃNDOLA AL SISTEMA V5 CON PLAYWRIGHT.

âœ… CARACTERÃSTICAS INTEGRADAS DEL PORT:
- Selectores exactos que funcionan: div[data-cnstrc-item-id]
- ExtracciÃ³n de data attributes
- Parsing de precios con regex especÃ­fico  
- Timing de scroll: 10s inicial + scroll completo + scroll a mitad
- Especificaciones tÃ©cnicas (storage, RAM, network, color)
- Sistema de rating y reviews
- Enlaces e imÃ¡genes
- Estructura de datos completa

ğŸ¯ OBJETIVO: 100% funcionalidad PORT en arquitectura V5 Playwright
"""

import asyncio
import logging
import re
import json
from typing import Dict, List, Optional, Any
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin

from playwright.async_api import Page

# Importaciones del sistema V5
try:
    from core.base_scraper import BaseScraperV5, ProductData, ScrapingResult
    from core.utils import *
except ImportError:
    # Fallback para testing independiente
    class BaseScraperV5:
        def __init__(self, retailer): 
            self.retailer = retailer
            self.logger = logging.getLogger(__name__)
        async def get_page(self): pass
    
    class ProductData:
        def __init__(self, **kwargs): 
            for k, v in kwargs.items(): 
                setattr(self, k, v)
    
    class ScrapingResult:
        def __init__(self, **kwargs): 
            for k, v in kwargs.items(): 
                setattr(self, k, v)

logger = logging.getLogger(__name__)

# ==========================================
# SELECTORES EXACTOS DEL PORT (FUNCIONALES)
# ==========================================

# Selector principal: div con data-cnstrc-item-id (el que SÃ FUNCIONA)
PRODUCT_CONTAINER_SELECTOR = 'div[data-cnstrc-item-id]'

# Selectores de precios (exactos del PORT)
PRICE_SELECTORS = {
    'current_price': 'span.ui-text-\\[13px\\].ui-leading-\\[15px\\].desktop\\:ui-text-lg, span[class*="ui-font-semibold"][class*="desktop:ui-font-medium"]',
    'old_price': 'span.ui-line-through.ui-font-semibold',
    'discount': 'div[data-testid="paris-label"][aria-label*="%"]',
    'brand': 'span.ui-font-semibold.ui-line-clamp-2.ui-text-\\[11px\\]',
    'rating_value': 'button[data-testid="star-rating-rating-value"]',
    'rating_count': 'button[data-testid="star-rating-total-rating"]'
}

# Links y metadatos
METADATA_SELECTORS = {
    'product_link': 'a[href]',
    'product_image': 'img[alt="Imagen de producto"]'
}

# Regex exactos del PORT para especificaciones
SPECS_REGEX = {
    'storage': r'(\d+)gb(?!\s+ram)',  # Storage (no RAM)
    'ram': r'(\d+)gb\s+ram',         # RAM especÃ­fico
    'network': r'(4g|5g)',           # Red mÃ³vil
    'color': r'(negro|blanco|azul|rojo|verde|gris|dorado|plateado|purpura|rosa)$'
}


class ParisScraperV5PortIntegrated(BaseScraperV5):
    """ğŸ¬ Scraper Paris V5 - LÃ“GICA PORT INTEGRADA COMPLETA"""
    
    def __init__(self):
        super().__init__("paris")
        
        # URLs exactas
        self.base_url = "https://www.paris.cl/tecnologia/celulares/"
        
        # ConfiguraciÃ³n de paginaciÃ³n centralizada
        self.pagination_config = self._load_pagination_config()
        
        # ConfiguraciÃ³n ULTRA-RÃPIDA optimizada
        self.config = {
            'initial_wait': 2,         # 2 segundos mÃ­nimos
            'scroll_wait': 1,          # 1 segundo despuÃ©s del scroll
            'mid_scroll_wait': 0.5,    # 0.5 segundos despuÃ©s del scroll a mitad
            'element_timeout': 3000,   # 3 segundos timeout por elemento
            'page_timeout': 30000,     # 30 segundos total optimizado
        }
        
        self.logger.info("ğŸ¬ Paris Scraper V5 - PORT INTEGRADO inicializado")

    def _load_pagination_config(self) -> Dict[str, Any]:
        """ğŸ“„ Cargar configuraciÃ³n de paginaciÃ³n desde config.json"""
        try:
            config_path = Path(__file__).parent.parent / "config.json"
            if not config_path.exists():
                self.logger.warning(f"âš ï¸ Config.json no encontrado en {config_path}")
                return {}
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            paris_config = config_data.get('retailers', {}).get('paris', {})
            pagination_config = paris_config.get('paginacion', {})
            
            if pagination_config:
                self.logger.info(f"âœ… ConfiguraciÃ³n de paginaciÃ³n cargada: {pagination_config.get('url_pattern', 'N/A')}")
                return pagination_config
            else:
                self.logger.warning("âš ï¸ No se encontrÃ³ configuraciÃ³n de paginaciÃ³n para Paris")
                return {}
                
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error cargando configuraciÃ³n de paginaciÃ³n: {e}")
            return {}

    async def scrape_category(
        self, 
        category: str = "celulares",
        max_products: int = 500,
        filters: Dict[str, Any] = None
    ) -> ScrapingResult:
        """ğŸ” Scraper con lÃ³gica PORT COMPLETA integrada"""
        
        start_time = datetime.now()
        session_id = f"paris_port_{category}_{int(start_time.timestamp())}"
        
        page = None
        try:
            self.logger.info(f"ğŸ” Scraping Paris PORT INTEGRADO - {category}: {self.base_url}")
            
            # Obtener pÃ¡gina
            page = await self.get_page()
            if not page:
                raise Exception("No se pudo obtener la pÃ¡gina")
            
            # Scraping con paginaciÃ³n usando configuraciÃ³n centralizada
            products = await self._scrape_with_pagination_port(page, max_products)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            result = ScrapingResult(
                success=True,
                products=products,
                total_found=len(products),
                execution_time=execution_time,
                session_id=session_id,
                source_url=self.base_url,
                retailer=self.retailer,
                category=category,
                timestamp=datetime.now(),
                metadata={
                    'scraping_method': 'port_logic_integrated',
                    'port_compatibility': True,
                    'extraction_method': 'playwright_port_selectors',
                    'success_rate': f"{len(products)}/{max_products}",
                    'pagination_used': len(self.pagination_config) > 0
                }
            )
            
            # Guardar JSON COMPLETO (esperando a que termine)
            await self._save_retailer_json_complete(products, session_id, execution_time)
            
            self.logger.info(f"âœ… Paris PORT INTEGRADO completado: {len(products)} productos en {execution_time:.1f}s")
            
            return result
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            
            self.logger.error(f"âŒ Error Paris PORT INTEGRADO categorÃ­a {category}: {e}")
            
            return ScrapingResult(
                success=False,
                products=[],
                total_found=0,
                execution_time=execution_time,
                session_id=session_id,
                source_url=self.base_url,
                retailer=self.retailer,
                category=category,
                timestamp=datetime.now(),
                error_message=str(e),
                metadata={'error_type': type(e).__name__, 'scraping_method': 'port_logic_integrated'}
            )
        
        finally:
            # Limpiar recursos de forma segura
            if page:
                try:
                    await page.close()
                except:
                    pass

    async def _scrape_with_pagination_port(self, initial_page: Page, max_products: int) -> List[ProductData]:
        """ğŸ“¦ Scraping PARALELO con 5 pÃ¡ginas simultÃ¡neas + detecciÃ³n automÃ¡tica + guardado periÃ³dico"""
        
        all_products = []
        current_batch_start = 1
        consecutive_empty_batches = 0
        
        # ConfiguraciÃ³n de detecciÃ³n de fin (desde config.json)
        max_empty_pages = self.pagination_config.get('empty_page_threshold', 2) if self.pagination_config else 2
        min_products_per_page = self.pagination_config.get('min_products_per_page', 5) if self.pagination_config else 5
        max_pages_config = self.pagination_config.get('max_pages', 999) if self.pagination_config else 999
        auto_stop_enabled = self.pagination_config.get('auto_stop', True) if self.pagination_config else True
        
        # ConfiguraciÃ³n de paralelismo
        batch_size = 5  # 5 pÃ¡ginas simultÃ¡neas
        
        self.logger.info(f"ğŸš€ Iniciando paginaciÃ³n PARALELA:")
        self.logger.info(f"   ğŸ“„ Max pÃ¡ginas: {max_pages_config}")
        self.logger.info(f"   âš¡ PÃ¡ginas paralelas: {batch_size}")
        self.logger.info(f"   ğŸ”š Auto-stop: {'Activado' if auto_stop_enabled else 'Desactivado'}")
        self.logger.info(f"   âš ï¸ Threshold pÃ¡ginas vacÃ­as: {max_empty_pages}")
        
        while len(all_products) < max_products and current_batch_start <= max_pages_config:
            # Preparar lote de pÃ¡ginas para procesar en paralelo
            batch_pages = []
            for i in range(batch_size):
                page_num = current_batch_start + i
                if page_num > max_pages_config:
                    break
                batch_pages.append(page_num)
            
            if not batch_pages:
                break
            
            self.logger.info(f"ğŸš€ Procesando lote PARALELO pÃ¡ginas {batch_pages[0]} a {batch_pages[-1]}")
            
            # Procesar pÃ¡ginas en paralelo
            batch_results = await self._process_pages_parallel(batch_pages)
            
            # Procesar resultados del lote
            batch_total_products = 0
            empty_pages_in_batch = 0
            
            for page_num, page_result in batch_results:
                if page_result is None:
                    empty_pages_in_batch += 1
                    continue
                
                page_products, page_status = page_result
                
                if page_status == "empty":
                    empty_pages_in_batch += 1
                    continue
                elif page_status == "error":
                    continue
                elif page_status == "success":
                    batch_total_products += len(page_products)
                    all_products.extend(page_products)
                    self.logger.info(f"âœ… PÃ¡gina {page_num}: {len(page_products)} productos")
            
            self.logger.info(f"ğŸ“Š Lote completado: {batch_total_products} productos de {len(batch_pages)} pÃ¡ginas")
            
            # Verificar condiciones de fin
            if auto_stop_enabled:
                # Si todas las pÃ¡ginas del lote estÃ¡n vacÃ­as
                if empty_pages_in_batch == len(batch_pages):
                    consecutive_empty_batches += 1
                    self.logger.warning(f"âš ï¸ Lote completamente vacÃ­o ({consecutive_empty_batches})")
                    
                    if consecutive_empty_batches >= 2:  # 2 lotes vacÃ­os = terminar
                        self.logger.info("ğŸ”š 2 lotes vacÃ­os consecutivos, terminando")
                        break
                else:
                    consecutive_empty_batches = 0
                
                # Verificar lÃ­mite de productos
                if len(all_products) >= max_products:
                    self.logger.info(f"ğŸ¯ LÃ­mite alcanzado: {len(all_products)}/{max_products}")
                    break
            
            # ğŸ’¾ Guardado periÃ³dico cada 10 lotes para prevenir pÃ©rdida de datos
            if len(all_products) > 0 and (current_batch_start % 50 == 1):  # Cada 50 pÃ¡ginas
                try:
                    import time
                    current_time = time.time()
                    await self._save_retailer_json_complete(all_products, f"periodic_{current_batch_start}", current_time)
                    self.logger.info(f"ğŸ’¾ Guardado periÃ³dico: {len(all_products)} productos salvados")
                except Exception as save_error:
                    self.logger.error(f"âš ï¸ Error en guardado periÃ³dico: {save_error}")
            
            # Siguiente lote
            current_batch_start += batch_size
            
            # Pausa mÃ­nima entre lotes
            await asyncio.sleep(0.1)
        
        # ğŸ›¡ï¸ Guardado final SEGURO - Asegurar que TODOS los productos se guarden
        if len(all_products) > 0:
            try:
                import time
                final_time = time.time()
                await self._save_retailer_json_complete(all_products, f"final_{len(all_products)}", final_time)
                self.logger.info(f"ğŸ’¾ GUARDADO FINAL EXITOSO: {len(all_products)} productos")
            except Exception as final_save_error:
                self.logger.error(f"ğŸš¨ ERROR CRÃTICO en guardado final: {final_save_error}")
                # Intentar guardado de emergencia
                try:
                    emergency_filename = f"paris_emergency_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                    emergency_data = {
                        "total_products": len(all_products),
                        "products": [prod.to_dict() for prod in all_products],
                        "emergency_save": True,
                        "timestamp": datetime.now().isoformat()
                    }
                    import json
                    with open(emergency_filename, 'w', encoding='utf-8') as f:
                        json.dump(emergency_data, f, ensure_ascii=False, indent=2)
                    self.logger.warning(f"âš ï¸ Guardado de emergencia en: {emergency_filename}")
                except:
                    self.logger.error("ğŸš¨ FALLO TOTAL del guardado - datos perdidos")
        
        # Limitar resultados finales
        final_products = all_products[:max_products]
        
        self.logger.info(f"ğŸ“¦ PaginaciÃ³n PARALELA completada:")
        self.logger.info(f"   ğŸ“„ PÃ¡ginas procesadas: hasta {current_batch_start - 1}")
        self.logger.info(f"   âš¡ Lotes procesados: {(current_batch_start - 1) // batch_size}")
        self.logger.info(f"   ğŸ¯ Productos extraÃ­dos: {len(final_products)}")
        self.logger.info(f"   ğŸ”š RazÃ³n de fin: {'LÃ­mite alcanzado' if len(all_products) >= max_products else 'No mÃ¡s productos'}")
        
        return final_products

    async def _process_pages_parallel(self, page_numbers: List[int]) -> List[tuple]:
        """ğŸš€ Procesar mÃºltiples pÃ¡ginas en paralelo usando asyncio.gather"""
        
        # Crear tareas para cada pÃ¡gina
        tasks = []
        for page_num in page_numbers:
            task = asyncio.create_task(self._scrape_single_page(page_num))
            tasks.append(task)
        
        # Ejecutar todas las tareas en paralelo
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Procesar resultados y manejar excepciones
        processed_results = []
        for i, result in enumerate(results):
            page_num = page_numbers[i]
            
            if isinstance(result, Exception):
                error_msg = str(result)
                if 'ERR_ABORTED' in error_msg:
                    self.logger.warning(f"ğŸ”„ PÃ¡gina {page_num} ERR_ABORTED - continuando con otras")
                else:
                    self.logger.error(f"âŒ Error pÃ¡gina {page_num}: {result}")
                processed_results.append((page_num, None))
            else:
                processed_results.append((page_num, result))
        
        return processed_results

    async def _scrape_single_page(self, page_num: int) -> tuple:
        """ğŸ“„ Scraper una pÃ¡gina individual (para usar en paralelo)"""
        
        page = None
        try:
            # Crear nueva pÃ¡gina para esta tarea
            page = await self.get_page()
            if not page:
                return ([], "error")
            
            # Construir URL
            page_url = self._build_page_url_centralized(page_num)
            
            # Navegar a la pÃ¡gina con manejo mejorado de ERR_ABORTED
            navigation_result = await self._navigate_with_error_detection(page, page_url)
            if not navigation_result:
                # Distinguir entre ERR_ABORTED (seguir) vs otros errores (detener)
                self.logger.info(f"ğŸ”„ NavegaciÃ³n fallÃ³ en pÃ¡gina {page_num} - marcando como vacÃ­a")
                return ([], "empty")  # Marcar como vacÃ­a en lugar de error para continuar
            
            # Verificar si es redirecciÃ³n a pÃ¡gina principal
            current_url = page.url
            if page_num > 1 and current_url == self.base_url:
                self.logger.info(f"ğŸ”š PÃ¡gina {page_num} redirigiÃ³ a principal")
                return ([], "empty")
            
            # Aplicar timing optimizado
            await self._apply_port_timing(page)
            
            # Extraer productos
            page_products = await self._extract_products_port_logic(page)
            
            if len(page_products) == 0:
                return ([], "empty")
            
            return (page_products, "success")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error pÃ¡gina {page_num}: {e}")
            return ([], "error")
        
        finally:
            # Cerrar pÃ¡gina especÃ­fica
            if page:
                try:
                    await page.close()
                except:
                    pass

    async def _save_retailer_json(self, products: List[ProductData], session_id: str, execution_time: float):
        """ğŸ’¾ Guardar resultados en JSON especÃ­fico por retailer con timestamp"""
        try:
            # Crear timestamp para el archivo
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Crear directorio especÃ­fico por retailer
            retailer_dir = Path(__file__).parent.parent / "resultados_json" / self.retailer
            retailer_dir.mkdir(parents=True, exist_ok=True)
            
            # Nombre del archivo con timestamp
            filename = f"{self.retailer}_productos_{timestamp}.json"
            filepath = retailer_dir / filename
            
            # Convertir ProductData a diccionarios serializables
            products_data = []
            for product in products:
                product_dict = {
                    'title': product.title,
                    'sku': product.sku,
                    'brand': product.brand,
                    'current_price': product.current_price,
                    'original_price': product.original_price,
                    'product_url': product.product_url,
                    'image_urls': product.image_urls,
                    'rating': product.rating,
                    'reviews_count': product.reviews_count,
                    'retailer': product.retailer,
                    'category': product.category,
                    'extraction_timestamp': product.extraction_timestamp.isoformat() if hasattr(product.extraction_timestamp, 'isoformat') else str(product.extraction_timestamp),
                    'additional_info': product.additional_info
                }
                products_data.append(product_dict)
            
            # Crear estructura JSON completa
            json_data = {
                'retailer': self.retailer,
                'extraction_timestamp': timestamp,
                'session_id': session_id,
                'execution_time_seconds': execution_time,
                'total_products': len(products),
                'extraction_method': 'port_logic_integrated_v5',
                'pagination_config': self.pagination_config,
                'source_url': self.base_url,
                'products': products_data,
                'extraction_summary': {
                    'brands_found': list(set(p.get('brand', '') for p in products_data if p.get('brand'))),
                    'price_range': {
                        'min_price': min((p.get('current_price', 0) for p in products_data if p.get('current_price', 0) > 0), default=0),
                        'max_price': max((p.get('current_price', 0) for p in products_data if p.get('current_price', 0) > 0), default=0),
                        'avg_price': sum(p.get('current_price', 0) for p in products_data if p.get('current_price', 0) > 0) / len([p for p in products_data if p.get('current_price', 0) > 0]) if products_data else 0
                    },
                    'discounts_found': len([p for p in products_data if p.get('additional_info', {}).get('discount_percent')]),
                    'products_with_specs': len([p for p in products_data if p.get('additional_info', {}).get('storage')])
                }
            }
            
            # Guardar archivo JSON
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ğŸ’¾ JSON guardado: {filepath}")
            self.logger.info(f"ğŸ“Š Resumen guardado:")
            self.logger.info(f"   ğŸ·ï¸ Marcas: {len(json_data['extraction_summary']['brands_found'])} diferentes")
            self.logger.info(f"   ğŸ’° Rango precios: ${json_data['extraction_summary']['price_range']['min_price']:,.0f} - ${json_data['extraction_summary']['price_range']['max_price']:,.0f}")
            self.logger.info(f"   ğŸ·ï¸ Descuentos: {json_data['extraction_summary']['discounts_found']} productos")
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error guardando JSON especÃ­fico: {e}")

    async def _navigate_with_error_detection(self, page: Page, url: str) -> bool:
        """ğŸ§­ NavegaciÃ³n con detecciÃ³n de errores y pÃ¡ginas invÃ¡lidas + manejo ERR_ABORTED"""
        try:
            response = await page.goto(url, wait_until='domcontentloaded', timeout=self.config['page_timeout'])
            
            # Verificar cÃ³digos de error HTTP
            if response and response.status >= 400:
                self.logger.warning(f"âš ï¸ CÃ³digo HTTP {response.status} en {url}")
                if response.status == 404:
                    self.logger.info("ğŸ”š PÃ¡gina 404 - fin de pÃ¡ginas alcanzado")
                    return False
                elif response.status >= 500:
                    self.logger.warning("ğŸ”„ Error del servidor, intentando continuar...")
            
            return True
            
        except Exception as e:
            error_msg = str(e)
            
            # Manejar especÃ­ficamente errores ERR_ABORTED
            if 'ERR_ABORTED' in error_msg:
                self.logger.warning(f"ğŸ”„ ERR_ABORTED detectado en {url} - continuando con otras pÃ¡ginas")
                # No retornar False para ERR_ABORTED, permitir continuar
                return False  # Esta pÃ¡gina fallÃ³ pero no detenemos el proceso completo
            
            # Manejar otros errores de timeout/navegaciÃ³n
            elif 'timeout' in error_msg.lower() or 'net::' in error_msg:
                self.logger.warning(f"ğŸ”„ Error de red en {url}: {error_msg} - continuando")
                return False  # Esta pÃ¡gina fallÃ³ pero seguimos
            
            # Otros errores mÃ¡s graves
            else:
                self.logger.error(f"ğŸ’¥ Error crÃ­tico navegando a {url}: {e}")
                return False

    async def _detect_end_of_pages(self, page: Page, page_num: int) -> bool:
        """ğŸ”š Detectar indicadores de fin de pÃ¡ginas usando mÃºltiples estrategias"""
        try:
            # Estrategia 1: Detectar mensajes de "no hay mÃ¡s resultados"
            no_results_selectors = [
                'div:has-text("No se encontraron productos")',
                'div:has-text("No hay mÃ¡s resultados")',
                'div:has-text("Sin resultados")',
                '.no-results',
                '.empty-results',
                '[data-testid*="no-results"]',
                'div:has-text("0 productos")'
            ]
            
            for selector in no_results_selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        self.logger.info(f"ğŸ”š Mensaje de fin detectado: '{text.strip()}'")
                        return True
                except:
                    continue
            
            # Estrategia 2: Detectar URL de redirecciÃ³n a pÃ¡gina principal
            current_url = page.url
            if 'page=' not in current_url and page_num > 1:
                self.logger.info(f"ğŸ”„ RedirecciÃ³n detectada a pÃ¡gina principal desde pÃ¡gina {page_num}")
                return True
            
            # Estrategia 3: Detectar tÃ­tulos de pÃ¡gina que indican fin
            try:
                title = await page.title()
                end_titles = ['404', 'pÃ¡gina no encontrada', 'error']
                if any(end_title in title.lower() for end_title in end_titles):
                    self.logger.info(f"ğŸ”š TÃ­tulo de fin detectado: '{title}'")
                    return True
            except:
                pass
            
            # Estrategia 4: Detectar controles de paginaciÃ³n
            pagination_indicators = await self._check_pagination_controls(page, page_num)
            if pagination_indicators:
                return True
                
            return False
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error en detecciÃ³n de fin: {e}")
            return False

    async def _check_pagination_controls(self, page: Page, current_page: int) -> bool:
        """ğŸ“„ Verificar controles de paginaciÃ³n para detectar fin"""
        try:
            # Buscar controles de paginaciÃ³n comunes
            pagination_selectors = [
                '.pagination',
                '[data-testid*="pagination"]',
                '.page-numbers',
                '.pager',
                'nav[aria-label*="paginaciÃ³n"]',
                'nav[aria-label*="pagination"]'
            ]
            
            for selector in pagination_selectors:
                try:
                    pagination = await page.query_selector(selector)
                    if pagination:
                        # Buscar botÃ³n "siguiente" deshabilitado
                        next_disabled = await pagination.query_selector('button[disabled]:has-text("Siguiente"), a[disabled]:has-text("Siguiente"), .disabled:has-text("Siguiente")')
                        if next_disabled:
                            self.logger.info("ğŸ”š BotÃ³n 'Siguiente' deshabilitado - fin de pÃ¡ginas")
                            return True
                        
                        # Verificar si la pÃ¡gina actual es la mÃ¡xima visible
                        page_numbers = await pagination.query_selector_all('a, button')
                        if page_numbers:
                            max_page = 0
                            for page_elem in page_numbers:
                                try:
                                    text = await page_elem.inner_text()
                                    if text.isdigit():
                                        max_page = max(max_page, int(text))
                                except:
                                    continue
                            
                            if max_page > 0 and current_page >= max_page:
                                self.logger.info(f"ğŸ”š PÃ¡gina actual ({current_page}) >= mÃ¡xima visible ({max_page})")
                                return True
                        
                        break
                except:
                    continue
                    
            return False
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error verificando controles de paginaciÃ³n: {e}")
            return False

    async def _verify_next_page_empty(self, page: Page, next_page_num: int) -> bool:
        """ğŸ” Verificar si la prÃ³xima pÃ¡gina estÃ¡ vacÃ­a (sin navegar completamente)"""
        try:
            # Construir URL de la prÃ³xima pÃ¡gina
            next_url = self._build_page_url_centralized(next_page_num)
            
            # Hacer una verificaciÃ³n rÃ¡pida (HEAD request simulado)
            current_url = page.url
            
            # NavegaciÃ³n rÃ¡pida sin timing completo
            try:
                await page.goto(next_url, wait_until='domcontentloaded', timeout=10000)
                await asyncio.sleep(2)  # Espera mÃ­nima
                
                # Verificar si hay contenedores de productos
                containers = await page.query_selector_all(PRODUCT_CONTAINER_SELECTOR)
                is_empty = len(containers) == 0
                
                # Volver a la pÃ¡gina anterior
                await page.goto(current_url, wait_until='domcontentloaded', timeout=10000)
                
                return is_empty
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Error verificando pÃ¡gina {next_page_num}: {e}")
                return True  # Asumir vacÃ­a si hay error
                
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error en verificaciÃ³n de pÃ¡gina siguiente: {e}")
            return False

    async def _detect_duplicate_pattern(self, products: List[ProductData]) -> bool:
        """ğŸ”„ Detectar patrones de productos duplicados (posible loop)"""
        try:
            if len(products) < 20:  # No hay suficientes productos para detectar patrones
                return False
            
            # Obtener Ãºltimos 20 productos
            recent_products = products[-20:]
            skus = [p.sku for p in recent_products if p.sku]
            
            # Verificar si hay mÃ¡s de 50% duplicados en los Ãºltimos productos
            if len(skus) > 10:
                unique_skus = set(skus)
                duplicate_ratio = 1 - (len(unique_skus) / len(skus))
                
                if duplicate_ratio > 0.5:
                    self.logger.warning(f"ğŸ”„ Alto ratio de duplicados detectado: {duplicate_ratio:.1%}")
                    return True
            
            # Verificar patrones de repeticiÃ³n en los Ãºltimos 10 productos
            if len(products) >= 10:
                last_10_skus = [p.sku for p in products[-10:] if p.sku]
                first_10_skus = [p.sku for p in products[:10] if p.sku]
                
                if len(last_10_skus) >= 5 and len(first_10_skus) >= 5:
                    common = set(last_10_skus) & set(first_10_skus)
                    if len(common) >= 3:
                        self.logger.warning(f"ğŸ”„ PatrÃ³n de repeticiÃ³n detectado: {len(common)} SKUs comunes")
                        return True
            
            return False
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Error detectando duplicados: {e}")
            return False

    def _build_page_url_centralized(self, page_num: int) -> str:
        """ğŸ”— Construir URL especÃ­fica para Paris con lÃ³gica correcta"""
        try:
            # LÃ“GICA ESPECÃFICA PARA PARIS:
            # PÃ¡gina 1: https://www.paris.cl/tecnologia/celulares/ (sin parÃ¡metros)
            # PÃ¡gina 2+: https://www.paris.cl/tecnologia/celulares/?page=2
            
            if page_num <= 1:
                # PÃ¡gina 1: URL base sin parÃ¡metros
                return self.base_url
            else:
                # PÃ¡gina 2+: agregar ?page=X
                base_clean = self.base_url.rstrip('/')  # Quitar / final si existe
                return f"{base_clean}/?page={page_num}"
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error construyendo URL de pÃ¡gina: {e}")
            # Fallback seguro
            if page_num <= 1:
                return self.base_url
            else:
                base_clean = self.base_url.rstrip('/')
                return f"{base_clean}/?page={page_num}"

    async def _apply_port_timing(self, page: Page):
        """âš¡ ULTRA-RÃPIDO: Timing mÃ­nimo optimizado para velocidad"""
        try:
            # 1. Espera mÃ­nima hasta detectar contenedores
            self.logger.info("âš¡ MODO ULTRA-RÃPIDO: Carga mÃ­nima...")
            
            # Esperar mÃ¡ximo 3 segundos para contenedores iniciales
            for _ in range(6):  # 6 x 0.5s = 3s mÃ¡ximo
                containers = await page.query_selector_all(PRODUCT_CONTAINER_SELECTOR)
                if len(containers) >= 3:  # Solo 3 productos mÃ­nimos
                    self.logger.info(f"âš¡ Carga detectada: {len(containers)} contenedores")
                    break
                await asyncio.sleep(0.5)
            
            # 2. Scroll rÃ¡pido Ãºnico al final
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
            await asyncio.sleep(1)  # Solo 1 segundo
            
            # 3. Scroll final a media pÃ¡gina
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight/2);")
            await asyncio.sleep(0.5)  # Solo medio segundo
            
            final_count = len(await page.query_selector_all(PRODUCT_CONTAINER_SELECTOR))
            self.logger.info(f"âš¡ RÃPIDO: {final_count} contenedores listos")
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error timing rÃ¡pido: {e}")
            # Fallback sÃºper rÃ¡pido
            await asyncio.sleep(2)
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
            await asyncio.sleep(1)

    async def _extract_products_port_logic(self, page: Page) -> List[ProductData]:
        """ğŸ“Š Extraer productos usando lÃ³gica EXACTA del PORT"""
        products = []
        
        try:
            # Buscar contenedores con data-cnstrc-item-id (selector PORT)
            containers = await page.query_selector_all(PRODUCT_CONTAINER_SELECTOR)
            
            self.logger.info(f"ğŸ” Contenedores con data-cnstrc-item-id encontrados: {len(containers)}")
            
            for container in containers:
                try:
                    # Extraer informaciÃ³n desde data attributes (como PORT)
                    product_code = await container.get_attribute('data-cnstrc-item-id') or ''
                    product_name = await container.get_attribute('data-cnstrc-item-name') or ''
                    price_from_data = await container.get_attribute('data-cnstrc-item-price') or ''
                    
                    product_name = product_name.strip()
                    
                    # Filtro de productos vÃ¡lidos
                    if not product_code or not product_name:
                        continue
                    
                    # Buscar link del producto
                    product_link = ""
                    link_element = await container.query_selector('a[href]')
                    if link_element:
                        href = await link_element.get_attribute('href') or ''
                        if href.startswith('/'):
                            product_link = f"https://www.paris.cl{href}"
                        elif href.startswith('http'):
                            product_link = href
                    
                    # Extraer precios usando selectores PORT
                    current_price_text = ""
                    current_price_numeric = None
                    old_price_text = ""
                    old_price_numeric = None
                    discount_percent = ""
                    
                    # Precio actual
                    current_price_elem = await container.query_selector(PRICE_SELECTORS['current_price'])
                    if current_price_elem:
                        current_price_text = await current_price_elem.inner_text()
                        current_price_text = current_price_text.strip()
                        price_match = re.search(r'\$?([\d.,]+)', current_price_text.replace('.', ''))
                        if price_match:
                            try:
                                current_price_numeric = int(price_match.group(1).replace(',', ''))
                            except:
                                pass
                    
                    # Precio anterior (con descuento)
                    old_price_elem = await container.query_selector(PRICE_SELECTORS['old_price'])
                    if old_price_elem:
                        old_price_text = await old_price_elem.inner_text()
                        old_price_text = old_price_text.strip()
                        price_match = re.search(r'\$?([\d.,]+)', old_price_text.replace('.', ''))
                        if price_match:
                            try:
                                old_price_numeric = int(price_match.group(1).replace(',', ''))
                            except:
                                pass
                    
                    # Descuento
                    discount_elem = await container.query_selector(PRICE_SELECTORS['discount'])
                    if discount_elem:
                        discount_percent = await discount_elem.get_attribute('aria-label') or ''
                    
                    # Marca
                    brand = ""
                    brand_elem = await container.query_selector(PRICE_SELECTORS['brand'])
                    if brand_elem:
                        brand = await brand_elem.inner_text()
                        brand = brand.strip()
                    
                    # Imagen
                    img_src = ""
                    img_element = await container.query_selector(METADATA_SELECTORS['product_image'])
                    if img_element:
                        # Obtener imagen de mayor resoluciÃ³n del srcset
                        srcset = await img_element.get_attribute('srcset') or ''
                        if srcset:
                            urls = re.findall(r'(https://[^\s]+)', srcset)
                            img_src = urls[-1] if urls else (await img_element.get_attribute('src') or '')
                        else:
                            img_src = await img_element.get_attribute('src') or ''
                    
                    # Rating y reviews
                    rating = "0"
                    rating_elem = await container.query_selector(PRICE_SELECTORS['rating_value'])
                    if rating_elem:
                        rating = await rating_elem.inner_text()
                        rating = rating.strip()
                    
                    reviews = "0"
                    reviews_elem = await container.query_selector(PRICE_SELECTORS['rating_count'])
                    if reviews_elem:
                        reviews = await reviews_elem.inner_text()
                        reviews = reviews.strip().replace('(', '').replace(')', '')
                    
                    # Extraer especificaciones usando regex PORT
                    storage = ""
                    ram = ""
                    network = ""
                    color = ""
                    
                    product_name_lower = product_name.lower()
                    
                    # Storage (primer nÃºmero + GB que no sea RAM)
                    storage_match = re.search(SPECS_REGEX['storage'], product_name_lower)
                    if storage_match:
                        storage = f"{storage_match.group(1)}GB"
                    
                    # RAM
                    ram_match = re.search(SPECS_REGEX['ram'], product_name_lower)
                    if ram_match:
                        ram = f"{ram_match.group(1)}GB"
                    
                    # Red mÃ³vil
                    network_match = re.search(SPECS_REGEX['network'], product_name_lower)
                    if network_match:
                        network = network_match.group(1).upper()
                    
                    # Color
                    color_match = re.search(SPECS_REGEX['color'], product_name_lower)
                    if color_match:
                        color = color_match.group(1).title()
                    
                    # Crear ProductData con estructura V5
                    product_data = ProductData(
                        title=product_name,
                        sku=product_code,
                        brand=brand,
                        current_price=float(current_price_numeric) if current_price_numeric else 0.0,
                        original_price=float(old_price_numeric) if old_price_numeric else 0.0,
                        product_url=product_link,
                        image_urls=[img_src] if img_src else [],
                        rating=float(rating) if rating.replace('.', '').isdigit() else 0.0,
                        reviews_count=int(reviews) if reviews.isdigit() else 0,
                        retailer="paris",
                        category="celulares",
                        additional_info={
                            'storage': storage,
                            'ram': ram,
                            'network': network,
                            'color': color,
                            'discount_percent': discount_percent,
                            'current_price_text': current_price_text,
                            'old_price_text': old_price_text,
                            'price_from_data': price_from_data,
                            'scraped_with': 'port_logic_integrated'
                        }
                    )
                    
                    products.append(product_data)
                    
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Error procesando contenedor: {e}")
                    continue
            
            self.logger.info(f"âœ… Productos extraÃ­dos con lÃ³gica PORT: {len(products)}")
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error en extracciÃ³n PORT: {e}")
        
        return products

    async def _save_retailer_json_complete(self, products: List[ProductData], session_id: str, execution_time: float):
        """ğŸ’¾ Guardar resultados COMPLETOS en JSON especÃ­fico por retailer con timestamp"""
        try:
            # Crear timestamp para el archivo
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Crear directorio especÃ­fico por retailer
            retailer_dir = Path(__file__).parent.parent / "resultados_json" / self.retailer
            retailer_dir.mkdir(parents=True, exist_ok=True)
            
            # Nombre del archivo con timestamp
            filename = f"{self.retailer}_productos_{timestamp}.json"
            filepath = retailer_dir / filename
            
            # Convertir ProductData a diccionarios serializables
            products_data = []
            for product in products:
                product_dict = {
                    'title': product.title,
                    'sku': product.sku,
                    'brand': product.brand,
                    'current_price': product.current_price,
                    'original_price': product.original_price,
                    'product_url': product.product_url,
                    'image_urls': product.image_urls,
                    'rating': product.rating,
                    'reviews_count': product.reviews_count,
                    'retailer': product.retailer,
                    'category': product.category,
                    'extraction_timestamp': product.extraction_timestamp.isoformat() if hasattr(product.extraction_timestamp, 'isoformat') else str(product.extraction_timestamp),
                    'additional_info': product.additional_info
                }
                products_data.append(product_dict)
            
            # Crear estructura JSON completa
            json_data = {
                'retailer': self.retailer,
                'extraction_timestamp': timestamp,
                'session_id': session_id,
                'execution_time_seconds': execution_time,
                'total_products': len(products),
                'extraction_method': 'port_logic_integrated_v5',
                'pagination_config': self.pagination_config,
                'source_url': self.base_url,
                'products': products_data,
                'extraction_summary': {
                    'brands_found': list(set(p.get('brand', '') for p in products_data if p.get('brand'))),
                    'price_range': {
                        'min_price': min((p.get('current_price', 0) for p in products_data if p.get('current_price', 0) > 0), default=0),
                        'max_price': max((p.get('current_price', 0) for p in products_data if p.get('current_price', 0) > 0), default=0),
                        'avg_price': sum(p.get('current_price', 0) for p in products_data if p.get('current_price', 0) > 0) / len([p for p in products_data if p.get('current_price', 0) > 0]) if products_data else 0
                    },
                    'discounts_found': len([p for p in products_data if p.get('additional_info', {}).get('discount_percent')]),
                    'products_with_specs': len([p for p in products_data if p.get('additional_info', {}).get('storage')])
                }
            }
            
            # Guardar archivo JSON
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ğŸ’¾ JSON guardado COMPLETO: {filepath}")
            self.logger.info(f"ğŸ“Š Resumen guardado:")
            self.logger.info(f"   ğŸ·ï¸ Marcas: {len(json_data['extraction_summary']['brands_found'])} diferentes")
            self.logger.info(f"   ğŸ’° Rango precios: ${json_data['extraction_summary']['price_range']['min_price']:,.0f} - ${json_data['extraction_summary']['price_range']['max_price']:,.0f}")
            self.logger.info(f"   ğŸ¯ Total productos guardados: {len(products_data)}")
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ Error guardando JSON completo: {e}")
            import traceback
            self.logger.error(traceback.format_exc())