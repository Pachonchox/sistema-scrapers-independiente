# ğŸ“‹ INSTRUCCIONES COMPLETAS - Sistema Scrapers Independiente V5

## ğŸš€ INSTALACIÃ“N RÃPIDA (3 pasos)

### 1ï¸âƒ£ Instalar Python (si no lo tienes)
```bash
# Descargar desde: https://www.python.org/downloads/
# AsegÃºrate de marcar "Add Python to PATH" durante instalaciÃ³n
```

### 2ï¸âƒ£ Instalar automÃ¡ticamente
```bash
# En Windows:
python setup.py

# En Linux/Mac:
python3 setup.py
```

### 3ï¸âƒ£ Ejecutar scrapers
```bash
# OpciÃ³n 1: Script automÃ¡tico (Windows)
EJECUTAR_SCRAPERS.bat

# OpciÃ³n 2: Comando directo
python run_scrapers_system.py

# OpciÃ³n 3: Scraper individual
python run_scrapers_system.py falabella
```

---

## ğŸ› ï¸ INSTALACIÃ“N MANUAL (si la automÃ¡tica falla)

### Paso 1: Dependencias
```bash
pip install playwright pandas openpyxl beautifulsoup4 lxml requests fake-useragent colorama
python -m playwright install chromium
```

### Paso 2: Directorios
```bash
mkdir resultados
mkdir logs
```

### Paso 3: Probar
```bash
python test_system.py
```

---

## ğŸ¯ USO DEL SISTEMA

### Ejecutar Todos los Scrapers
```bash
python run_scrapers_system.py
```

### Ejecutar Scraper Individual
```bash
python run_scrapers_system.py falabella
python run_scrapers_system.py paris
python run_scrapers_system.py ripley
python run_scrapers_system.py hites
python run_scrapers_system.py abcdin
python run_scrapers_system.py mercadolibre
```

### Probar el Sistema
```bash
python test_system.py
```

---

## âš™ï¸ CONFIGURACIÃ“N

### Editar config.json
```json
{
  "sistema": {
    "max_workers": 1,          // 1=secuencial, >1=paralelo
    "export_excel": true,      // Exportar Excel individuales
    "export_unified": true     // Crear reporte unificado
  },
  "retailers": {
    "falabella": {
      "activo": true,          // true/false para activar/desactivar
      "max_productos": 100     // MÃ¡ximo productos a extraer
    }
  }
}
```

### Configurar Retailers
- **Activar/Desactivar**: Cambiar `"activo": true/false`
- **LÃ­mite productos**: Cambiar `"max_productos": 50`
- **Delays**: Modificar `"delay_min"` y `"delay_max"`

---

## ğŸ“Š RESULTADOS

### Archivos Generados
```
resultados/
â”œâ”€â”€ falabella_productos_20240905_143022.xlsx    // Individual
â”œâ”€â”€ paris_productos_20240905_143045.xlsx        // Individual
â”œâ”€â”€ ripley_productos_20240905_143108.xlsx       // Individual
â”œâ”€â”€ resumen_scraping_20240905_143200.xlsx       // Unificado
â””â”€â”€ scraping_20240905.log                       // Log completo
```

### Contenido Excel
- **Nombre**: Nombre del producto
- **Precio**: Precio extraÃ­do
- **Link**: URL del producto
- **Retailer**: Tienda de origen
- **Timestamp**: Fecha/hora extracciÃ³n

---

## ğŸ›¡ï¸ CARACTERÃSTICAS ANTI-DETECCIÃ“N

### âœ… Implementadas
- RotaciÃ³n automÃ¡tica User Agents
- Delays aleatorios entre requests
- Headers realistas de navegador
- OcultaciÃ³n de webdriver properties
- Reintentos automÃ¡ticos en caso de error
- ConfiguraciÃ³n especial por retailer

### ğŸ¯ Especiales por Retailer
- **Ripley**: Navegador visible off-screen + scroll obligatorio
- **AbcDin**: Wait for DOM content loaded
- **Todos**: Timeouts y delays personalizados

---

## ğŸ”§ TROUBLESHOOTING

### âŒ Error: "Python no encontrado"
```bash
# Instalar Python desde python.org
# Asegurarse de marcar "Add to PATH"
# Reiniciar terminal
```

### âŒ Error: "playwright no encontrado"
```bash
pip install playwright
python -m playwright install chromium
```

### âŒ Error: "No products extracted"
```bash
# 1. Verificar conexiÃ³n internet
# 2. Probar con menos productos (max_productos: 10)
# 3. Verificar logs en resultados/
# 4. Ejecutar test_system.py
```

### âŒ Error: "Config file not found"
```bash
# Verificar que config.json existe en el directorio
# Ejecutar desde el directorio scrapers_independientes/
```

### âš ï¸ Pocos productos extraÃ­dos
```bash
# Normal: sitios web cambian frecuentemente
# SoluciÃ³n: Actualizar selectores en config.json
# Ver logs para detalles especÃ­ficos
```

---

## ğŸ“ˆ OPTIMIZACIÃ“N

### Rendimiento
- **Paralelo**: `max_workers: 3` (mÃ¡ximo recomendado)
- **Secuencial**: `max_workers: 1` (mÃ¡s estable)
- **Productos**: Reducir `max_productos` si hay timeouts

### Estabilidad
- Usar `headless: true` para mejor rendimiento
- Aumentar timeouts si hay errores de red
- Reducir workers en conexiones lentas

---

## ğŸ“ LOGS Y DEBUGGING

### Ver Logs
```bash
# Archivo de log
cat resultados/scraping_YYYYMMDD.log

# En tiempo real (Windows)
type resultados\scraping_YYYYMMDD.log

# Ãšltimas lÃ­neas
tail -f resultados/scraping_YYYYMMDD.log
```

### Niveles de Log
- âœ… **INFO**: Operaciones normales
- âš ï¸ **WARNING**: Problemas menores
- âŒ **ERROR**: Errores crÃ­ticos
- ğŸ” **DEBUG**: InformaciÃ³n detallada

---

## ğŸ”„ MANTENIMIENTO

### Actualizar Selectores
Si un scraper deja de funcionar:

1. Verificar la pÃ¡gina web manualmente
2. Inspeccionar elementos (F12)
3. Actualizar selectores en `config.json`
4. Probar con `python test_system.py`

### Backup ConfiguraciÃ³n
```bash
# Hacer backup antes de cambios
cp config.json config.json.backup
```

---

## ğŸŒŸ CARACTERÃSTICAS AVANZADAS

### Ejecutar en Horarios EspecÃ­ficos
```bash
# Windows (Task Scheduler)
schtasks /create /tn "Scrapers" /tr "python run_scrapers_system.py" /sc daily /st 09:00

# Linux (Cron)
echo "0 9 * * * cd /path/to/scrapers && python3 run_scrapers_system.py" | crontab -
```

### Monitoreo de Resultados
```bash
# Contar productos por dÃ­a
ls resultados/*productos*.xlsx | wc -l

# Ver tamaÃ±os de archivos
ls -lh resultados/
```

---

## ğŸ“ SOPORTE

### Archivos Importantes para Debug
- `config.json` - ConfiguraciÃ³n
- `resultados/scraping_YYYYMMDD.log` - Logs
- `test_system.py` - DiagnÃ³stico

### InformaciÃ³n Ãštil para Reportar Problemas
- VersiÃ³n Python: `python --version`
- Sistema operativo
- Error exacto del log
- ConfiguraciÃ³n usada
- Scraper especÃ­fico que falla

---

## ğŸ‰ Â¡LISTO PARA USAR!

El sistema estÃ¡ completamente independiente y funcional. 
Ejecuta `python run_scrapers_system.py` y disfruta de tu sistema de scrapers automatizado. ğŸ˜Š