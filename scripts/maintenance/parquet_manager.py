# -*- coding: utf-8 -*-
"""
ğŸ—„ï¸ Parquet Manager - Utilidades de GestiÃ³n
==========================================

Script de utilidades para gestionar los respaldos Parquet:
- Ver estadÃ­sticas de respaldos
- Listar archivos por retailer/fecha
- Limpiar archivos antiguos
- Convertir Parquet a CSV/Excel
- AnÃ¡lisis de datos respaldados
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd

# Add project root to Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

try:
    from core.logging_config import get_system_logger
    from core.parquet_backup_system import ParquetBackupSystem, get_backup_stats, list_recent_backups
    logger = get_system_logger("parquet_manager")
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger("parquet_manager")

class ParquetManager:
    """Manager para operaciones con respaldos Parquet"""
    
    def __init__(self):
        self.system = ParquetBackupSystem()
    
    def show_stats(self):
        """Mostrar estadÃ­sticas de respaldos"""
        logger.info("ğŸ“Š ESTADÃSTICAS DE RESPALDOS PARQUET")
        logger.info("=" * 50)
        
        stats = get_backup_stats()
        
        if "error" in stats:
            logger.error(f"âŒ Error: {stats['error']}")
            return
        
        logger.info(f"ğŸª Total retailers: {stats['total_retailers']}")
        logger.info(f"ğŸ“ Total archivos: {stats['total_files']}")
        logger.info(f"ğŸ’¾ TamaÃ±o total: {stats['total_size_mb']:.2f} MB")
        
        if stats.get('oldest_backup'):
            logger.info(f"ğŸ“… Respaldo mÃ¡s antiguo: {stats['oldest_backup']}")
        
        if stats.get('newest_backup'):
            logger.info(f"ğŸ“… Respaldo mÃ¡s reciente: {stats['newest_backup']}")
        
        logger.info("\nğŸª DETALLES POR RETAILER:")
        for retailer, retailer_stats in stats['retailers'].items():
            logger.info(f"  {retailer}:")
            logger.info(f"    ğŸ“ Archivos: {retailer_stats['files']}")
            logger.info(f"    ğŸ’¾ TamaÃ±o: {retailer_stats['size_mb']:.2f} MB")
            logger.info(f"    ğŸ“… Fechas: {len(retailer_stats['dates'])} dÃ­as")
    
    def list_backups(self, retailer=None, days=7):
        """Listar respaldos recientes"""
        logger.info(f"ğŸ“‹ RESPALDOS RECIENTES ({days} dÃ­as)")
        logger.info("=" * 50)
        
        if retailer:
            logger.info(f"ğŸª Filtro: {retailer}")
        
        backups = list_recent_backups(retailer, days)
        
        if not backups:
            logger.info("ğŸ“­ No se encontraron respaldos")
            return
        
        logger.info(f"ğŸ“¦ Encontrados {len(backups)} respaldos:")
        
        for backup in backups:
            logger.info(f"  ğŸ“ {backup['retailer']}/{backup['date']}/{backup['file']}")
            logger.info(f"      ğŸ’¾ {backup['size_mb']} MB | ğŸ“… {backup['modified']}")
    
    def convert_to_csv(self, parquet_file, output_file=None):
        """Convertir Parquet a CSV"""
        logger.info(f"ğŸ”„ Convirtiendo Parquet a CSV: {parquet_file}")
        
        try:
            # Leer Parquet
            df = self.system.read_backup(parquet_file)
            
            if df.empty:
                logger.error("âŒ El archivo Parquet estÃ¡ vacÃ­o")
                return
            
            # Generar nombre de salida
            if not output_file:
                parquet_path = Path(parquet_file)
                output_file = parquet_path.with_suffix('.csv')
            
            # Guardar CSV
            df.to_csv(output_file, index=False, encoding='utf-8')
            
            logger.info(f"âœ… CSV creado: {output_file}")
            logger.info(f"ğŸ“Š {len(df)} filas, {len(df.columns)} columnas")
            
        except Exception as e:
            logger.error(f"âŒ Error convirtiendo a CSV: {e}")
    
    def convert_to_excel(self, parquet_file, output_file=None):
        """Convertir Parquet a Excel"""
        logger.info(f"ğŸ“Š Convirtiendo Parquet a Excel: {parquet_file}")
        
        try:
            # Leer Parquet
            df = self.system.read_backup(parquet_file)
            
            if df.empty:
                logger.error("âŒ El archivo Parquet estÃ¡ vacÃ­o")
                return
            
            # Generar nombre de salida
            if not output_file:
                parquet_path = Path(parquet_file)
                output_file = parquet_path.with_suffix('.xlsx')
            
            # Guardar Excel
            df.to_excel(output_file, index=False, engine='openpyxl')
            
            logger.info(f"âœ… Excel creado: {output_file}")
            logger.info(f"ğŸ“Š {len(df)} filas, {len(df.columns)} columnas")
            
        except Exception as e:
            logger.error(f"âŒ Error convirtiendo a Excel: {e}")
    
    def analyze_file(self, parquet_file):
        """AnÃ¡lizar archivo Parquet"""
        logger.info(f"ğŸ” ANÃLISIS DE ARCHIVO PARQUET")
        logger.info("=" * 50)
        logger.info(f"ğŸ“ Archivo: {parquet_file}")
        
        try:
            df = self.system.read_backup(parquet_file)
            
            if df.empty:
                logger.error("âŒ El archivo estÃ¡ vacÃ­o")
                return
            
            logger.info(f"ğŸ“Š Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas")
            logger.info(f"ğŸ’¾ TamaÃ±o en memoria: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB")
            
            # InformaciÃ³n de columnas
            logger.info("\nğŸ“‹ COLUMNAS:")
            for col in df.columns:
                dtype = df[col].dtype
                null_count = df[col].isnull().sum()
                unique_count = df[col].nunique()
                logger.info(f"  {col}: {dtype} | Nulos: {null_count} | Ãšnicos: {unique_count}")
            
            # Retailers Ãºnicos
            if 'retailer' in df.columns:
                retailers = df['retailer'].unique()
                logger.info(f"\nğŸª Retailers: {list(retailers)}")
            
            # CategorÃ­as Ãºnicas
            if 'categoria' in df.columns:
                categories = df['categoria'].unique()
                logger.info(f"ğŸ“‚ CategorÃ­as: {list(categories)}")
            
            # Marcas mÃ¡s comunes
            if 'marca' in df.columns:
                top_brands = df['marca'].value_counts().head(5)
                logger.info(f"\nğŸ·ï¸ TOP 5 MARCAS:")
                for brand, count in top_brands.items():
                    logger.info(f"  {brand}: {count} productos")
            
            # Rangos de precios
            price_cols = ['precio_normal', 'precio_oferta']
            for col in price_cols:
                if col in df.columns:
                    prices = pd.to_numeric(df[col], errors='coerce')
                    prices = prices.dropna()
                    if not prices.empty:
                        logger.info(f"\nğŸ’° {col.upper()}:")
                        logger.info(f"  Min: ${prices.min():,.0f}")
                        logger.info(f"  Max: ${prices.max():,.0f}")
                        logger.info(f"  Promedio: ${prices.mean():,.0f}")
                        logger.info(f"  Mediana: ${prices.median():,.0f}")
            
        except Exception as e:
            logger.error(f"âŒ Error analizando archivo: {e}")
    
    def cleanup_old(self, days=30):
        """Limpiar archivos antiguos"""
        logger.info(f"ğŸ§¹ LIMPIANDO RESPALDOS ANTIGUOS (>{days} dÃ­as)")
        logger.info("=" * 50)
        
        result = self.system.cleanup_old_backups(days)
        
        if result.get('success'):
            logger.info(f"âœ… Limpieza completada:")
            logger.info(f"ğŸ—‘ï¸ Carpetas eliminadas: {result['deleted_folders']}")
            logger.info(f"ğŸ’¾ Espacio liberado: {result['freed_space_mb']:.2f} MB")
            logger.info(f"ğŸ“… Fecha lÃ­mite: {result['cutoff_date']}")
        else:
            logger.error(f"âŒ Error en limpieza: {result.get('error')}")
    
    def backup_info(self, retailer, date=None):
        """InformaciÃ³n de respaldos de un retailer en una fecha"""
        if date is None:
            date = datetime.now().strftime("%Y-%m-%d")
        
        logger.info(f"â„¹ï¸ INFORMACIÃ“N DE RESPALDOS")
        logger.info("=" * 50)
        logger.info(f"ğŸª Retailer: {retailer}")
        logger.info(f"ğŸ“… Fecha: {date}")
        
        retailer_path = self.system.base_path / retailer / date
        
        if not retailer_path.exists():
            logger.warning(f"âš ï¸ No se encontraron respaldos para {retailer} en {date}")
            return
        
        # Archivos Parquet
        parquet_files = list(retailer_path.glob("*.parquet"))
        logger.info(f"\nğŸ“¦ ARCHIVOS PARQUET: {len(parquet_files)}")
        
        total_size = 0
        for pf in parquet_files:
            size_mb = pf.stat().st_size / (1024 * 1024)
            total_size += size_mb
            logger.info(f"  ğŸ“ {pf.name}: {size_mb:.2f} MB")
        
        logger.info(f"ğŸ’¾ TamaÃ±o total: {total_size:.2f} MB")
        
        # Metadata
        metadata_files = list(retailer_path.glob("*.json"))
        if metadata_files:
            logger.info(f"\nğŸ“‹ METADATA: {len(metadata_files)} archivos")
            for mf in metadata_files:
                logger.info(f"  ğŸ“„ {mf.name}")

def main():
    parser = argparse.ArgumentParser(description="Parquet Manager - GestiÃ³n de respaldos")
    subparsers = parser.add_subparsers(dest='command', help='Comandos disponibles')
    
    # Stats command
    subparsers.add_parser('stats', help='Mostrar estadÃ­sticas de respaldos')
    
    # List command
    list_parser = subparsers.add_parser('list', help='Listar respaldos recientes')
    list_parser.add_argument('--retailer', help='Filtrar por retailer')
    list_parser.add_argument('--days', type=int, default=7, help='DÃ­as hacia atrÃ¡s (default: 7)')
    
    # Convert commands
    csv_parser = subparsers.add_parser('to-csv', help='Convertir Parquet a CSV')
    csv_parser.add_argument('parquet_file', help='Archivo Parquet a convertir')
    csv_parser.add_argument('--output', help='Archivo CSV de salida')
    
    excel_parser = subparsers.add_parser('to-excel', help='Convertir Parquet a Excel')
    excel_parser.add_argument('parquet_file', help='Archivo Parquet a convertir')
    excel_parser.add_argument('--output', help='Archivo Excel de salida')
    
    # Analyze command
    analyze_parser = subparsers.add_parser('analyze', help='Analizar archivo Parquet')
    analyze_parser.add_argument('parquet_file', help='Archivo Parquet a analizar')
    
    # Cleanup command
    cleanup_parser = subparsers.add_parser('cleanup', help='Limpiar archivos antiguos')
    cleanup_parser.add_argument('--days', type=int, default=30, help='DÃ­as a mantener (default: 30)')
    
    # Info command
    info_parser = subparsers.add_parser('info', help='InformaciÃ³n de respaldos')
    info_parser.add_argument('retailer', help='Nombre del retailer')
    info_parser.add_argument('--date', help='Fecha (YYYY-MM-DD, default: hoy)')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = ParquetManager()
    
    # Ejecutar comando
    if args.command == 'stats':
        manager.show_stats()
    elif args.command == 'list':
        manager.list_backups(args.retailer, args.days)
    elif args.command == 'to-csv':
        manager.convert_to_csv(args.parquet_file, args.output)
    elif args.command == 'to-excel':
        manager.convert_to_excel(args.parquet_file, args.output)
    elif args.command == 'analyze':
        manager.analyze_file(args.parquet_file)
    elif args.command == 'cleanup':
        manager.cleanup_old(args.days)
    elif args.command == 'info':
        manager.backup_info(args.retailer, args.date)

if __name__ == "__main__":
    main()