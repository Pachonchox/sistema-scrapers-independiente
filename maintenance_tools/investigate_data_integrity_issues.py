#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸ” INVESTIGACIÃ“N DE PROBLEMAS DE INTEGRIDAD DE DATOS
===================================================

AnÃ¡lisis especÃ­fico de los 3 problemas crÃ­ticos encontrados:
1. ğŸ”— Link duplicado (76 ocurrencias)
2. ğŸ’¸ Precios invÃ¡lidos (oferta > precio normal)
3. ğŸ“ CÃ³digos con formato incorrecto

Este script identifica las causas raÃ­z y propone soluciones.

Autor: Sistema V5 Production
Fecha: 04/09/2025
"""

import os
import sys
import psycopg2
import psycopg2.extras
from datetime import datetime
from pathlib import Path
import json
from typing import Dict, List, Any
import re

# Configurar soporte completo de emojis
sys.path.append(str(Path(__file__).parent))
from portable_orchestrator_v5.core.emoji_support import force_emoji_support
force_emoji_support()

# Cargar configuraciÃ³n
from dotenv import load_dotenv
load_dotenv()

class DataIntegrityIssueInvestigator:
    """ğŸ” Investigador de problemas especÃ­ficos de integridad de datos"""
    
    def __init__(self):
        """Inicializar investigador"""
        print("ğŸ” Inicializando Investigador de Problemas de Integridad...")
        
        # ParÃ¡metros de conexiÃ³n
        self.conn_params = {
            'host': os.getenv('PGHOST', 'localhost'),
            'port': int(os.getenv('PGPORT', '5432')),
            'database': os.getenv('PGDATABASE', 'orchestrator'),
            'user': os.getenv('PGUSER', 'postgres'),
            'password': os.getenv('PGPASSWORD', 'postgres')
        }
        
        try:
            self.conn = psycopg2.connect(**self.conn_params)
            self.conn.autocommit = True
            self.cursor = self.conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
            print(f"âœ… Conectado a PostgreSQL: {self.conn_params['host']}:{self.conn_params['port']}")
        except Exception as e:
            print(f"âŒ Error conectando a PostgreSQL: {e}")
            raise
        
        # Resultados de investigaciÃ³n
        self.findings = {
            'duplicate_links': {},
            'invalid_prices': {},
            'invalid_codes': {},
            'root_causes': [],
            'solutions': []
        }
    
    def investigate_duplicate_links(self):
        """ğŸ”— 1. Investigar links duplicados (76 ocurrencias)"""
        print("\n" + "ğŸ”—" + "="*80 + "ğŸ”—")
        print("ğŸ” INVESTIGANDO LINKS DUPLICADOS (76 OCURRENCIAS)")
        print("ğŸ”—" + "="*80 + "ğŸ”—")
        
        # Encontrar el link especÃ­fico con 76 duplicados
        self.cursor.execute("""
            SELECT link, COUNT(*) as count,
                   array_agg(codigo_interno) as codigos,
                   array_agg(nombre) as nombres,
                   array_agg(retailer) as retailers,
                   array_agg(created_at) as fechas_creacion
            FROM master_productos
            WHERE link IS NOT NULL AND link != ''
            GROUP BY link
            HAVING COUNT(*) > 10
            ORDER BY COUNT(*) DESC
            LIMIT 5
        """)
        
        duplicate_links = self.cursor.fetchall()
        
        print(f"ğŸ” Encontrados {len(duplicate_links)} links con mÃ¡s de 10 duplicados:")
        
        for dup in duplicate_links:
            print(f"\nğŸ“Š ANÃLISIS DEL LINK:")
            print(f"   ğŸ”— Link: {dup['link']}")
            print(f"   ğŸ“¦ Duplicados: {dup['count']}")
            print(f"   ğŸª Retailers: {set(dup['retailers'])}")
            print(f"   ğŸ“… Rango fechas: {min(dup['fechas_creacion'])} - {max(dup['fechas_creacion'])}")
            
            # Analizar patrones en los nombres
            nombres_unicos = set(dup['nombres'])
            print(f"   ğŸ“ Nombres Ãºnicos: {len(nombres_unicos)}")
            
            if len(nombres_unicos) <= 5:
                for nombre in list(nombres_unicos)[:5]:
                    print(f"      â€¢ {nombre[:80]}...")
            
            # Analizar cÃ³digos internos
            print(f"   ğŸ”§ CÃ³digos afectados:")
            for i, codigo in enumerate(dup['codigos'][:5]):
                print(f"      {i+1}. {codigo}")
            
            if dup['count'] == 76:  # El caso especÃ­fico
                self.findings['duplicate_links']['main_case'] = {
                    'link': dup['link'],
                    'count': dup['count'],
                    'codigos': dup['codigos'],
                    'retailers': list(set(dup['retailers'])),
                    'nombres_unicos': len(nombres_unicos),
                    'fecha_primera': min(dup['fechas_creacion']),
                    'fecha_ultima': max(dup['fechas_creacion'])
                }
        
        # Analizar causa raÃ­z del link duplicado
        print(f"\nğŸ” ANÃLISIS DE CAUSA RAÃZ - LINKS DUPLICADOS:")
        
        main_case = self.findings['duplicate_links'].get('main_case')
        if main_case:
            link_pattern = main_case['link']
            
            # Verificar si es un link genÃ©rico o placeholder
            generic_patterns = [
                'javascript:void(0)',
                '#',
                'about:blank',
                'null',
                'undefined',
                '',
                'N/A',
                'no-link'
            ]
            
            is_generic = any(pattern in link_pattern.lower() for pattern in generic_patterns)
            
            if is_generic:
                cause = "ğŸ¯ CAUSA RAÃZ: Link genÃ©rico/placeholder usado cuando el scraper no encuentra URL vÃ¡lida"
                solution = "Mejorar validaciÃ³n de URLs en scrapers y usar NULL en lugar de placeholders"
            else:
                cause = "ğŸ¯ CAUSA RAÃZ: Mismo producto extraÃ­do mÃºltiples veces con la misma URL"
                solution = "Implementar deduplicaciÃ³n por URL antes de insertar en base de datos"
            
            print(f"   {cause}")
            self.findings['root_causes'].append(cause)
            self.findings['solutions'].append(solution)
        
        print(f"\nğŸ’¡ RECOMENDACIÃ“N INMEDIATA:")
        print(f"   1. Revisar scrapers que generan URLs genÃ©ricas")
        print(f"   2. Implementar constraint UNIQUE en link (con manejo de NULL)")
        print(f"   3. Agregar validaciÃ³n de URL en proceso de scraping")
    
    def investigate_invalid_prices(self):
        """ğŸ’¸ 2. Investigar precios invÃ¡lidos (oferta > precio normal)"""
        print("\n" + "ğŸ’¸" + "="*80 + "ğŸ’¸")
        print("ğŸ” INVESTIGANDO PRECIOS INVÃLIDOS (OFERTA > PRECIO NORMAL)")
        print("ğŸ’¸" + "="*80 + "ğŸ’¸")
        
        # Encontrar casos especÃ­ficos de precios invÃ¡lidos
        self.cursor.execute("""
            SELECT p.codigo_interno, p.nombre, p.retailer, p.marca, p.categoria,
                   pr.fecha, pr.precio_normal, pr.precio_oferta, pr.precio_tarjeta,
                   pr.timestamp_creacion,
                   (pr.precio_oferta - pr.precio_normal) as diferencia_oferta,
                   ((pr.precio_oferta - pr.precio_normal) / pr.precio_normal * 100) as porcentaje_incremento
            FROM master_productos p
            JOIN master_precios pr ON p.codigo_interno = pr.codigo_interno
            WHERE pr.precio_oferta > pr.precio_normal
            AND pr.precio_oferta IS NOT NULL
            ORDER BY porcentaje_incremento DESC
            LIMIT 20
        """)
        
        invalid_prices = self.cursor.fetchall()
        
        print(f"ğŸ” Encontrados {len(invalid_prices)} casos de precios invÃ¡lidos:")
        
        total_cases = 0
        retailer_patterns = {}
        category_patterns = {}
        
        for case in invalid_prices:
            total_cases += 1
            
            print(f"\nğŸ“Š CASO {total_cases}:")
            print(f"   ğŸ“¦ Producto: {case['codigo_interno']}")
            print(f"   ğŸ“ Nombre: {case['nombre'][:60]}...")
            print(f"   ğŸª Retailer: {case['retailer']}")
            print(f"   ğŸ“‚ CategorÃ­a: {case['categoria']}")
            print(f"   ğŸ“… Fecha: {case['fecha']}")
            print(f"   ğŸ’° Normal: ${case['precio_normal']:,.0f}")
            print(f"   ğŸ”º Oferta: ${case['precio_oferta']:,.0f}")
            print(f"   ğŸ“ˆ Incremento: {case['porcentaje_incremento']:.1f}%")
            print(f"   ğŸ• Creado: {case['timestamp_creacion']}")
            
            # Acumular patrones por retailer
            retailer = case['retailer']
            if retailer not in retailer_patterns:
                retailer_patterns[retailer] = []
            retailer_patterns[retailer].append({
                'codigo': case['codigo_internal'],
                'incremento': case['porcentaje_incremento'],
                'diferencia': case['diferencia_oferta']
            })
            
            # Acumular patrones por categorÃ­a
            categoria = case['categoria'] or 'Sin categorÃ­a'
            if categoria not in category_patterns:
                category_patterns[categoria] = 0
            category_patterns[categoria] += 1
        
        # AnÃ¡lisis de patrones
        print(f"\nğŸ” ANÃLISIS DE PATRONES - PRECIOS INVÃLIDOS:")
        
        print(f"ğŸ“Š POR RETAILER:")
        for retailer, casos in retailer_patterns.items():
            avg_increment = sum(c['incremento'] for c in casos) / len(casos)
            print(f"   ğŸª {retailer}: {len(casos)} casos, incremento promedio: {avg_increment:.1f}%")
        
        print(f"ğŸ“Š POR CATEGORÃA:")
        for categoria, count in category_patterns.items():
            print(f"   ğŸ“‚ {categoria}: {count} casos")
        
        # Determinar causa raÃ­z
        print(f"\nğŸ¯ ANÃLISIS DE CAUSA RAÃZ - PRECIOS INVÃLIDOS:")
        
        # Verificar si hay patrÃ³n temporal
        self.cursor.execute("""
            SELECT DATE(timestamp_creacion) as fecha,
                   COUNT(*) as casos_invalidos
            FROM master_precios
            WHERE precio_oferta > precio_normal
            AND precio_oferta IS NOT NULL
            GROUP BY DATE(timestamp_creacion)
            ORDER BY fecha DESC
        """)
        
        temporal_pattern = self.cursor.fetchall()
        
        if temporal_pattern:
            print(f"ğŸ“… PatrÃ³n temporal de casos invÃ¡lidos:")
            for pattern in temporal_pattern:
                print(f"   {pattern['fecha']}: {pattern['casos_invalidos']} casos")
        
        # Posibles causas
        possible_causes = [
            "ğŸ¯ CAUSA 1: Error en scraping - extrayendo precio con descuento como 'normal'",
            "ğŸ¯ CAUSA 2: Problema en mapeo de campos - precio_normal y precio_oferta intercambiados",
            "ğŸ¯ CAUSA 3: Precios con formatos especiales no procesados correctamente",
            "ğŸ¯ CAUSA 4: Promociones especiales donde 'oferta' incluye beneficios adicionales"
        ]
        
        for cause in possible_causes:
            print(f"   {cause}")
            
        self.findings['root_causes'].extend(possible_causes)
        self.findings['invalid_prices'] = {
            'total_cases': total_cases,
            'by_retailer': retailer_patterns,
            'by_category': category_patterns,
            'temporal_pattern': temporal_pattern
        }
    
    def investigate_invalid_codes(self):
        """ğŸ“ 3. Investigar cÃ³digos con formato incorrecto"""
        print("\n" + "ğŸ“" + "="*80 + "ğŸ“")
        print("ğŸ” INVESTIGANDO CÃ“DIGOS CON FORMATO INCORRECTO")
        print("ğŸ“" + "="*80 + "ğŸ“")
        
        # Formato correcto esperado: CL-BRAND-MODEL-SPEC-RET-SEQ
        print("ğŸ“‹ Formato esperado: CL-BRAND-MODEL-SPEC-RET-SEQ")
        
        # Encontrar cÃ³digos con formato incorrecto
        self.cursor.execute("""
            SELECT codigo_interno, nombre, retailer, marca, categoria, created_at
            FROM master_productos
            WHERE codigo_interno NOT LIKE 'CL-%-%-%-%-%'
            ORDER BY created_at DESC
            LIMIT 50
        """)
        
        invalid_codes = self.cursor.fetchall()
        
        print(f"ğŸ” Encontrados {len(invalid_codes)} cÃ³digos con formato incorrecto:")
        
        # Analizar patrones de cÃ³digos incorrectos
        pattern_analysis = {
            'missing_cl': 0,
            'insufficient_parts': 0,
            'too_many_parts': 0,
            'empty_parts': 0,
            'special_chars': 0
        }
        
        retailer_issues = {}
        
        for i, code_info in enumerate(invalid_codes[:20]):  # Solo mostrar primeros 20
            codigo = code_info['codigo_interno']
            
            print(f"\nğŸ“¦ CÃ“DIGO INVÃLIDO {i+1}:")
            print(f"   ğŸ”§ CÃ³digo: {codigo}")
            print(f"   ğŸ“ Producto: {code_info['nombre'][:50]}...")
            print(f"   ğŸª Retailer: {code_info['retailer']}")
            print(f"   ğŸ·ï¸ Marca: {code_info['marca']}")
            print(f"   ğŸ“‚ CategorÃ­a: {code_info['categoria']}")
            print(f"   ğŸ“… Creado: {code_info['created_at']}")
            
            # Analizar el patrÃ³n especÃ­fico
            parts = codigo.split('-')
            
            if not codigo.startswith('CL-'):
                pattern_analysis['missing_cl'] += 1
                print(f"   âŒ Problema: No empieza con 'CL-'")
            
            if len(parts) < 6:
                pattern_analysis['insufficient_parts'] += 1
                print(f"   âŒ Problema: Partes insuficientes ({len(parts)}/6)")
            elif len(parts) > 6:
                pattern_analysis['too_many_parts'] += 1
                print(f"   âŒ Problema: Demasiadas partes ({len(parts)}/6)")
            
            if any(part == '' for part in parts):
                pattern_analysis['empty_parts'] += 1
                print(f"   âŒ Problema: Partes vacÃ­as")
            
            if re.search(r'[^A-Z0-9\-]', codigo):
                pattern_analysis['special_chars'] += 1
                print(f"   âŒ Problema: Caracteres especiales")
            
            # Acumular por retailer
            retailer = code_info['retailer']
            if retailer not in retailer_issues:
                retailer_issues[retailer] = 0
            retailer_issues[retailer] += 1
        
        # Resumen de patrones
        print(f"\nğŸ“Š RESUMEN DE PATRONES - CÃ“DIGOS INVÃLIDOS:")
        print(f"   ğŸ“¦ Total cÃ³digos invÃ¡lidos: {len(invalid_codes)}")
        print(f"   âŒ Sin prefijo 'CL-': {pattern_analysis['missing_cl']}")
        print(f"   ğŸ“ Partes insuficientes: {pattern_analysis['insufficient_parts']}")
        print(f"   ğŸ“ Demasiadas partes: {pattern_analysis['too_many_parts']}")
        print(f"   ğŸ”³ Partes vacÃ­as: {pattern_analysis['empty_parts']}")
        print(f"   ğŸ”£ Caracteres especiales: {pattern_analysis['special_chars']}")
        
        print(f"\nğŸª POR RETAILER:")
        for retailer, count in retailer_issues.items():
            print(f"   {retailer}: {count} cÃ³digos invÃ¡lidos")
        
        # Verificar lÃ³gica de generaciÃ³n de cÃ³digos
        print(f"\nğŸ” ANÃLISIS DE CAUSA RAÃZ - CÃ“DIGOS INVÃLIDOS:")
        
        root_causes = [
            "ğŸ¯ CAUSA 1: Algoritmo de generaciÃ³n no maneja correctamente marcas/modelos con caracteres especiales",
            "ğŸ¯ CAUSA 2: Campos vacÃ­os o NULL no se reemplazan adecuadamente en el formato",
            "ğŸ¯ CAUSA 3: Secuencias hexadecimales muy largas que rompen el formato",
            "ğŸ¯ CAUSA 4: ValidaciÃ³n insuficiente antes de insertar en base de datos"
        ]
        
        for cause in root_causes:
            print(f"   {cause}")
        
        self.findings['root_causes'].extend(root_causes)
        self.findings['invalid_codes'] = {
            'total_count': len(invalid_codes),
            'pattern_analysis': pattern_analysis,
            'retailer_breakdown': retailer_issues,
            'sample_codes': [c['codigo_interno'] for c in invalid_codes[:10]]
        }
    
    def create_prevention_plan(self):
        """ğŸ“‹ 4. Crear plan detallado de prevenciÃ³n"""
        print("\n" + "ğŸ“‹" + "="*80 + "ğŸ“‹")
        print("ğŸ“‹ PLAN DETALLADO DE PREVENCIÃ“N DE PROBLEMAS")
        print("ğŸ“‹" + "="*80 + "ğŸ“‹")
        
        prevention_plan = {
            "immediate_actions": [],
            "validation_improvements": [],
            "database_constraints": [],
            "monitoring_systems": [],
            "process_improvements": []
        }
        
        # 1. Acciones Inmediatas
        print(f"\nğŸš¨ 1. ACCIONES INMEDIATAS (PRÃ“XIMAS 24H):")
        
        immediate_actions = [
            "ğŸ”— Limpiar links duplicados existentes con UPDATE/DELETE",
            "ğŸ’¸ Corregir precios invÃ¡lidos intercambiando normal â†” oferta donde sea lÃ³gico",
            "ğŸ“ Regenerar cÃ³digos internos invÃ¡lidos con algoritmo mejorado",
            "ğŸ›¡ï¸ Implementar validaciÃ³n temporal en scrapers para prevenir nuevos casos"
        ]
        
        for action in immediate_actions:
            print(f"   {action}")
            prevention_plan["immediate_actions"].append(action)
        
        # 2. Mejoras de ValidaciÃ³n
        print(f"\nğŸ›¡ï¸ 2. MEJORAS DE VALIDACIÃ“N EN SCRAPERS:")
        
        validation_improvements = [
            "ğŸ”— URL Validation: Validar que links sean URLs vÃ¡lidas o usar NULL",
            "ğŸ’° Price Logic: Validar que precio_oferta â‰¤ precio_normal siempre",
            "ğŸ“ Code Format: Validar formato CL-X-X-X-X-X antes de insertar",
            "ğŸ” Duplicate Check: Verificar duplicados por URL antes de insertar",
            "ğŸ“Š Data Sanitization: Limpiar caracteres especiales en campos texto"
        ]
        
        for improvement in validation_improvements:
            print(f"   {improvement}")
            prevention_plan["validation_improvements"].append(improvement)
        
        # 3. Constraints de Base de Datos
        print(f"\nğŸ—„ï¸ 3. CONSTRAINTS DE BASE DE DATOS:")
        
        db_constraints = [
            "UNIQUE constraint en link (ignorando NULL)",
            "CHECK constraint para precio_oferta â‰¤ precio_normal",
            "CHECK constraint para formato de codigo_interno con regex",
            "TRIGGER para validaciÃ³n automÃ¡tica en INSERT/UPDATE",
            "PARTIAL INDEX en links no nulos para mejor rendimiento"
        ]
        
        for constraint in db_constraints:
            print(f"   â€¢ {constraint}")
            prevention_plan["database_constraints"].append(constraint)
        
        # 4. Sistema de Monitoreo
        print(f"\nğŸ“Š 4. SISTEMA DE MONITOREO CONTINUO:")
        
        monitoring_systems = [
            "ğŸ” Daily Audit: Script diario que detecte problemas automÃ¡ticamente",
            "ğŸ“§ Alert System: Notificaciones cuando se detecten anomalÃ­as",
            "ğŸ“ˆ Dashboard: MÃ©tricas de calidad de datos en tiempo real",
            "ğŸ¥ Health Check: Endpoint de salud que incluya integridad de datos",
            "ğŸ“‹ Weekly Report: Reporte semanal de calidad de datos"
        ]
        
        for system in monitoring_systems:
            print(f"   {system}")
            prevention_plan["monitoring_systems"].append(system)
        
        # 5. Mejoras de Proceso
        print(f"\nğŸ”„ 5. MEJORAS DE PROCESO DE SCRAPING:")
        
        process_improvements = [
            "ğŸ¯ Staging Area: Ãrea temporal para validar datos antes de insertar",
            "ğŸ”„ Rollback System: Capacidad de deshacer inserts problemÃ¡ticos",
            "ğŸ“ Data Pipeline: Pipeline con validaciones en mÃºltiples etapas",
            "ğŸ§ª Testing Framework: Tests automÃ¡ticos para validar scrapers",
            "ğŸ“Š Quality Metrics: KPIs de calidad de datos por retailer"
        ]
        
        for improvement in process_improvements:
            print(f"   {improvement}")
            prevention_plan["process_improvements"].append(improvement)
        
        # Timeline de implementaciÃ³n
        print(f"\nâ° 6. TIMELINE DE IMPLEMENTACIÃ“N:")
        
        timeline = [
            ("DÃ­a 1", "Acciones inmediatas + constraints bÃ¡sicos"),
            ("Semana 1", "Mejoras de validaciÃ³n en scrapers"),
            ("Semana 2", "Sistema de monitoreo bÃ¡sico"),
            ("Mes 1", "Pipeline completo + dashboard"),
            ("Mes 2", "Testing framework + automatizaciÃ³n completa")
        ]
        
        for period, tasks in timeline:
            print(f"   ğŸ“… {period}: {tasks}")
        
        # Guardar plan en archivo
        prevention_plan["timeline"] = timeline
        prevention_plan["creation_date"] = datetime.now().isoformat()
        
        plan_file = f"data_integrity_prevention_plan_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(plan_file, 'w', encoding='utf-8') as f:
            json.dump(prevention_plan, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ’¾ PLAN GUARDADO EN: {plan_file}")
        
        return prevention_plan
    
    def generate_sql_fixes(self):
        """ğŸ› ï¸ Generar SQL para correcciones inmediatas"""
        print("\n" + "ğŸ› ï¸" + "="*80 + "ğŸ› ï¸")
        print("ğŸ› ï¸ GENERANDO SQL PARA CORRECCIONES INMEDIATAS")
        print("ğŸ› ï¸" + "="*80 + "ğŸ› ï¸")
        
        sql_fixes = []
        
        # 1. SQL para limpiar links duplicados
        sql_fixes.append({
            "name": "Limpiar Links Duplicados",
            "sql": """
-- 1. Limpiar links duplicados manteniendo solo el mÃ¡s reciente
WITH duplicated_links AS (
    SELECT link, 
           codigo_interno,
           created_at,
           ROW_NUMBER() OVER (PARTITION BY link ORDER BY created_at DESC) as rn
    FROM master_productos
    WHERE link IS NOT NULL 
    AND link != ''
    AND link IN (
        SELECT link 
        FROM master_productos 
        WHERE link IS NOT NULL 
        GROUP BY link 
        HAVING COUNT(*) > 1
    )
)
UPDATE master_productos 
SET link = NULL 
WHERE codigo_interno IN (
    SELECT codigo_interno 
    FROM duplicated_links 
    WHERE rn > 1
);
            """,
            "description": "Mantiene solo el registro mÃ¡s reciente para cada link duplicado"
        })
        
        # 2. SQL para corregir precios invÃ¡lidos
        sql_fixes.append({
            "name": "Corregir Precios InvÃ¡lidos",
            "sql": """
-- 2. Intercambiar precio_normal y precio_oferta cuando oferta > normal
UPDATE master_precios 
SET 
    precio_normal = precio_oferta,
    precio_oferta = precio_normal
WHERE precio_oferta > precio_normal 
AND precio_oferta IS NOT NULL
AND precio_normal IS NOT NULL;

-- Agregar comentario de correcciÃ³n
UPDATE master_precios 
SET observaciones = COALESCE(observaciones || '; ', '') || 'Precios intercambiados por correcciÃ³n automÃ¡tica'
WHERE precio_oferta > precio_normal 
AND precio_oferta IS NOT NULL;
            """,
            "description": "Intercambia precios normal y oferta cuando la lÃ³gica estÃ¡ invertida"
        })
        
        # 3. SQL para agregar constraints
        sql_fixes.append({
            "name": "Agregar Constraints de ValidaciÃ³n",
            "sql": """
-- 3. Agregar constraints de validaciÃ³n
-- UNIQUE constraint para links (ignorando NULL)
CREATE UNIQUE INDEX CONCURRENTLY idx_unique_link 
ON master_productos (link) 
WHERE link IS NOT NULL AND link != '';

-- CHECK constraint para precios
ALTER TABLE master_precios 
ADD CONSTRAINT chk_precio_oferta_valido 
CHECK (precio_oferta IS NULL OR precio_oferta <= precio_normal);

-- CHECK constraint para formato de cÃ³digo interno
ALTER TABLE master_productos 
ADD CONSTRAINT chk_codigo_formato 
CHECK (codigo_interno ~ '^CL-[A-Z0-9]+-[A-Z0-9]+-[A-Z0-9]+-[A-Z]+-[A-Z0-9]+$');
            """,
            "description": "Agrega constraints para prevenir problemas futuros"
        })
        
        # 4. SQL para monitoreo
        sql_fixes.append({
            "name": "Crear Vistas de Monitoreo",
            "sql": """
-- 4. Crear vistas para monitoreo continuo
CREATE OR REPLACE VIEW v_data_quality_dashboard AS
SELECT 
    'Links Duplicados' as issue_type,
    COUNT(*) as count
FROM (
    SELECT link 
    FROM master_productos 
    WHERE link IS NOT NULL 
    GROUP BY link 
    HAVING COUNT(*) > 1
) duplicates

UNION ALL

SELECT 
    'Precios InvÃ¡lidos' as issue_type,
    COUNT(*) as count
FROM master_precios 
WHERE precio_oferta > precio_normal

UNION ALL

SELECT 
    'CÃ³digos InvÃ¡lidos' as issue_type,
    COUNT(*) as count
FROM master_productos 
WHERE codigo_interno NOT LIKE 'CL-%-%-%-%-%';

-- Vista para productos sin precios
CREATE OR REPLACE VIEW v_productos_sin_precios AS
SELECT p.codigo_interno, p.nombre, p.retailer, p.created_at
FROM master_productos p
LEFT JOIN master_precios pr ON p.codigo_interno = pr.codigo_interno
WHERE pr.codigo_interno IS NULL;
            """,
            "description": "Crea vistas para monitoreo continuo de calidad de datos"
        })
        
        # Guardar SQLs en archivo
        sql_file = f"data_integrity_fixes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql"
        
        with open(sql_file, 'w', encoding='utf-8') as f:
            f.write("-- SQL PARA CORRECCIONES DE INTEGRIDAD DE DATOS\n")
            f.write(f"-- Generado: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            for fix in sql_fixes:
                f.write(f"-- {fix['name']}\n")
                f.write(f"-- {fix['description']}\n")
                f.write(fix['sql'])
                f.write("\n\n")
        
        print(f"ğŸ’¾ SQL GUARDADO EN: {sql_file}")
        
        for fix in sql_fixes:
            print(f"\nğŸ› ï¸ {fix['name']}:")
            print(f"   ğŸ“ {fix['description']}")
        
        return sql_fixes
    
    def run_investigation(self):
        """ğŸš€ Ejecutar investigaciÃ³n completa"""
        print("ğŸ”" + "="*80 + "ğŸ”")
        print("ğŸš€ INVESTIGACIÃ“N COMPLETA DE PROBLEMAS DE INTEGRIDAD")
        print("ğŸ”" + "="*80 + "ğŸ”")
        
        investigation_steps = [
            ("ğŸ”— Links Duplicados", self.investigate_duplicate_links),
            ("ğŸ’¸ Precios InvÃ¡lidos", self.investigate_invalid_prices),
            ("ğŸ“ CÃ³digos Incorrectos", self.investigate_invalid_codes),
            ("ğŸ“‹ Plan de PrevenciÃ³n", self.create_prevention_plan),
            ("ğŸ› ï¸ SQL Fixes", self.generate_sql_fixes)
        ]
        
        for step_name, step_func in investigation_steps:
            try:
                print(f"\nâ³ Ejecutando: {step_name}...")
                step_func()
                print(f"âœ… {step_name} completado")
            except Exception as e:
                print(f"âŒ Error en {step_name}: {e}")
                import traceback
                print(f"ğŸ” Detalle: {traceback.format_exc()}")
        
        print("\nğŸ‰ INVESTIGACIÃ“N COMPLETA FINALIZADA")
        print("ğŸ“‹ RESUMEN DE ARCHIVOS GENERADOS:")
        print("   ğŸ“„ Plan de prevenciÃ³n: data_integrity_prevention_plan_*.json")
        print("   ğŸ› ï¸ Correcciones SQL: data_integrity_fixes_*.sql")
    
    def close(self):
        """ğŸ”š Cerrar conexiones"""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()
        print("ğŸ”š Conexiones cerradas")

def main():
    """ğŸš€ FunciÃ³n principal"""
    print("ğŸ” Iniciando InvestigaciÃ³n de Problemas de Integridad...")
    
    investigator = DataIntegrityIssueInvestigator()
    
    try:
        investigator.run_investigation()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ InvestigaciÃ³n interrumpida por el usuario")
    except Exception as e:
        print(f"\nâŒ Error crÃ­tico durante investigaciÃ³n: {e}")
        import traceback
        traceback.print_exc()
    finally:
        investigator.close()

if __name__ == "__main__":
    main()