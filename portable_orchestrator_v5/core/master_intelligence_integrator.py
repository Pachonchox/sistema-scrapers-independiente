#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
üß† INTEGRADOR MAESTRO DE INTELIGENCIA V5
=======================================

Integrador central que conecta todos los sistemas de inteligencia avanzada
con el orquestador V5 para operaci√≥n continua no supervisada y ultra-optimizada.

Sistemas Integrados:
- üß† Redis Intelligence System - An√°lisis de volatilidad y predicciones
- üöÄ Intelligent Cache Manager - Cache multi-nivel con ML
- ‚ö° Scraping Frequency Optimizer - Optimizaci√≥n autom√°tica de frecuencias
- üéöÔ∏è Advanced Tier Manager - Gesti√≥n inteligente de tiers
- üé≠ Anti-Detection System - Patrones humanos y evasi√≥n
- üìä Real-time Analytics - M√©tricas y dashboard en tiempo real

Caracter√≠sticas profesionales:
- üîÑ Integraci√≥n transparente con sistema V5 existente
- üìà Auto-optimizaci√≥n continua sin supervisi√≥n humana
- üéØ Toma de decisiones basada en ML y datos hist√≥ricos
- üìä Dashboard de m√©tricas en tiempo real
- üõ°Ô∏è Recuperaci√≥n autom√°tica ante fallos
- ‚ö° Rendimiento ultra-optimizado para operaci√≥n 24/7

Autor: Sistema V5 Production
Fecha: 03/09/2025
"""

import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Callable, Union
from dataclasses import dataclass, field, asdict
from pathlib import Path
import threading
import time
import os

# Imports de los sistemas de inteligencia
from .redis_intelligence_system import RedisIntelligenceSystem
from .intelligent_cache_manager import IntelligentCacheManager, intelligent_cache
from .scraping_frequency_optimizer import ScrapingFrequencyOptimizer
from .emoji_support import force_emoji_support

force_emoji_support()
logger = logging.getLogger(__name__)


@dataclass
class IntelligenceMetrics:
    """M√©tricas consolidadas del sistema de inteligencia"""
    # M√©tricas de rendimiento general
    total_operations: int = 0
    successful_operations: int = 0
    failed_operations: int = 0
    avg_response_time_ms: float = 0.0
    
    # Cache inteligente
    cache_hit_rate: float = 0.0
    cache_memory_usage_mb: float = 0.0
    cache_predictions_accuracy: float = 0.0
    
    # Optimizaci√≥n de frecuencias
    profiles_optimized: int = 0
    avg_frequency_reduction_pct: float = 0.0
    resource_savings_pct: float = 0.0
    
    # An√°lisis de volatilidad
    products_analyzed: int = 0
    high_volatility_products: int = 0
    prediction_confidence_avg: float = 0.0
    
    # Sistema general
    intelligence_score: float = 0.0  # 0-1, score general de inteligencia
    optimization_effectiveness: float = 0.0
    system_uptime_hours: float = 0.0
    
    last_updated: datetime = field(default_factory=datetime.now)


@dataclass
class IntelligenceDecision:
    """Decisi√≥n tomada por el sistema de inteligencia"""
    decision_type: str  # 'scraping_frequency', 'cache_strategy', 'tier_assignment', etc.
    target: str  # retailer:product_id o similar
    action: str
    confidence: float  # 0-1
    reasoning: str
    expected_impact: Dict[str, Any]
    created_at: datetime = field(default_factory=datetime.now)


class MasterIntelligenceIntegrator:
    """
    üß† Integrador Maestro de todos los Sistemas de Inteligencia
    
    Coordina y optimiza autom√°ticamente todos los aspectos del scraping
    usando inteligencia artificial y an√°lisis de datos en tiempo real
    """
    
    def __init__(self, redis_url: str = "redis://localhost:6379/0",
                 cache_memory_mb: int = 200):
        """
        Inicializar integrador maestro
        
        Args:
            redis_url: URL de conexi√≥n Redis
            cache_memory_mb: Memoria para cache inteligente
        """
        self.redis_url = redis_url
        self.cache_memory_mb = cache_memory_mb
        
        # Sistemas de inteligencia
        self.intelligence_system: Optional[RedisIntelligenceSystem] = None
        self.cache_manager: Optional[IntelligentCacheManager] = None
        self.frequency_optimizer: Optional[ScrapingFrequencyOptimizer] = None
        
        # Estado del integrador
        self.initialized = False
        self.running = False
        self.start_time = datetime.now()
        
        # Configuraci√≥n
        self.config = {
            'intelligence_enabled': True,
            'cache_enabled': True,
            'frequency_optimization_enabled': True,
            'real_time_decisions': True,
            'auto_recovery_enabled': True,
            
            # Intervalos de operaci√≥n
            'metrics_update_interval_seconds': 30,
            'decision_making_interval_seconds': 60,
            'health_check_interval_seconds': 120,
            'optimization_interval_hours': 4,
            
            # Thresholds de rendimiento
            'min_acceptable_performance_score': 0.7,
            'emergency_optimization_threshold': 0.5,
            'auto_recovery_threshold': 3,  # Fallos consecutivos
            
            # Configuraci√≥n de decisiones
            'min_decision_confidence': 0.6,
            'max_concurrent_optimizations': 5,
            'decision_history_limit': 1000
        }
        
        # M√©tricas y monitoreo
        self.metrics = IntelligenceMetrics()
        self.decision_history: List[IntelligenceDecision] = []
        self.health_status = {
            'overall_health': 'unknown',
            'component_health': {},
            'last_health_check': None,
            'consecutive_failures': 0
        }
        
        # Callbacks para integraci√≥n
        self.scraping_callbacks: List[Callable] = []
        self.decision_callbacks: List[Callable] = []
        
        # Tasks de fondo
        self.background_tasks: List[asyncio.Task] = []
        
        logger.info("üß† MasterIntelligenceIntegrator inicializado")
    
    async def initialize(self):
        """üöÄ Inicializar todos los sistemas de inteligencia"""
        try:
            logger.info("üöÄ Inicializando Master Intelligence Integrator...")
            
            # 1. Inicializar Redis Intelligence System
            logger.info("üß† Inicializando Redis Intelligence System...")
            self.intelligence_system = RedisIntelligenceSystem(self.redis_url)
            await self.intelligence_system.initialize()
            
            # 2. Inicializar Intelligent Cache Manager
            logger.info("üöÄ Inicializando Intelligent Cache Manager...")
            self.cache_manager = IntelligentCacheManager(
                redis_url=self.redis_url,
                max_memory_mb=self.cache_memory_mb
            )
            await self.cache_manager.initialize()
            
            # 3. Inicializar Scraping Frequency Optimizer
            logger.info("‚ö° Inicializando Scraping Frequency Optimizer...")
            self.frequency_optimizer = ScrapingFrequencyOptimizer(
                self.intelligence_system,
                self.cache_manager
            )
            await self.frequency_optimizer.initialize()
            
            # 4. Configurar integraci√≥n entre sistemas
            await self._setup_system_integration()
            
            # 5. Iniciar tasks de fondo
            await self._start_background_tasks()
            
            self.initialized = True
            self.running = True
            
            logger.info("‚úÖ Master Intelligence Integrator completamente inicializado")
            
            # Realizar verificaci√≥n inicial de salud
            await self._perform_health_check()
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando Master Intelligence Integrator: {e}")
            await self._emergency_shutdown()
            raise
    
    async def _setup_system_integration(self):
        """üîó Configurar integraci√≥n entre sistemas"""
        try:
            # Configurar callbacks entre sistemas para comunicaci√≥n
            # (En un sistema m√°s complejo, aqu√≠ se establecer√≠an los enlaces)
            
            logger.info("üîó Integraci√≥n entre sistemas configurada")
            
        except Exception as e:
            logger.error(f"‚ùå Error configurando integraci√≥n: {e}")
    
    async def _start_background_tasks(self):
        """üîÑ Iniciar tasks de fondo"""
        try:
            # Task para actualizaci√≥n de m√©tricas
            self.background_tasks.append(
                asyncio.create_task(self._metrics_update_loop())
            )
            
            # Task para toma de decisiones inteligentes
            self.background_tasks.append(
                asyncio.create_task(self._intelligent_decision_loop())
            )
            
            # Task para monitoreo de salud
            self.background_tasks.append(
                asyncio.create_task(self._health_monitoring_loop())
            )
            
            logger.info(f"üîÑ {len(self.background_tasks)} tasks de fondo iniciados")
            
        except Exception as e:
            logger.error(f"‚ùå Error iniciando tasks de fondo: {e}")
    
    async def analyze_scraping_request(self, retailer: str, product_id: str, 
                                     category: str = None) -> Dict[str, Any]:
        """
        üéØ Analizar petici√≥n de scraping y proporcionar recomendaciones inteligentes
        
        Args:
            retailer: Nombre del retailer
            product_id: ID del producto
            category: Categor√≠a del producto
            
        Returns:
            An√°lisis completo con recomendaciones
        """
        try:
            if not self.initialized:
                return {'error': 'Sistema no inicializado'}
            
            analysis_start_time = time.time()
            
            # 1. Verificar cache inteligente
            cache_key = f"scraping_analysis:{retailer}:{product_id}"
            cached_result, cache_level = await self.cache_manager.get(cache_key)
            
            if cache_level != 'MISS':
                self.metrics.cache_hit_rate = (self.metrics.cache_hit_rate * 0.9) + (1.0 * 0.1)
                return {
                    'source': 'cache',
                    'cache_level': cache_level,
                    **cached_result
                }
            
            # 2. An√°lisis de volatilidad
            volatility_analysis = await self._analyze_product_volatility(retailer, product_id)
            
            # 3. Optimizaci√≥n de frecuencia
            frequency_recommendation = await self._get_frequency_recommendation(
                retailer, product_id, volatility_analysis
            )
            
            # 4. An√°lisis de valor comercial
            commercial_value = await self._assess_commercial_value(retailer, product_id, category)
            
            # 5. Recomendaci√≥n de tier
            tier_recommendation = self._recommend_tier(volatility_analysis, commercial_value)
            
            # 6. Compilar an√°lisis completo
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'retailer': retailer,
                'product_id': product_id,
                'category': category,
                'volatility_analysis': volatility_analysis,
                'frequency_recommendation': frequency_recommendation,
                'commercial_value': commercial_value,
                'tier_recommendation': tier_recommendation,
                'confidence_score': self._calculate_analysis_confidence(volatility_analysis, commercial_value),
                'processing_time_ms': (time.time() - analysis_start_time) * 1000
            }
            
            # 7. Cachear resultado
            cache_ttl = 300  # 5 minutos base
            volatility_factor = volatility_analysis.get('volatility_score', 0.5)
            dynamic_ttl = int(cache_ttl * (1.0 - volatility_factor * 0.5))
            
            await self.cache_manager.set(
                cache_key, analysis_result,
                ttl=dynamic_ttl,
                volatility_score=volatility_factor,
                tags=['scraping_analysis', retailer, category or 'unknown']
            )
            
            # 8. Registrar decisi√≥n
            await self._record_decision(
                decision_type='scraping_analysis',
                target=f"{retailer}:{product_id}",
                action=f"tier_{tier_recommendation['recommended_tier']}",
                confidence=analysis_result['confidence_score'],
                reasoning=f"Volatilidad: {volatility_factor:.2f}, Valor: {commercial_value['score']:.2f}",
                expected_impact={'resource_optimization': frequency_recommendation.get('savings_pct', 0)}
            )
            
            self.metrics.total_operations += 1
            self.metrics.successful_operations += 1
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"‚ùå Error analizando petici√≥n de scraping: {e}")
            self.metrics.failed_operations += 1
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}
    
    async def _analyze_product_volatility(self, retailer: str, product_id: str) -> Dict[str, Any]:
        """üìä Analizar volatilidad del producto"""
        try:
            # Obtener historial de cambios desde Redis Intelligence
            change_key = f"intelligence:price_changes:{retailer}:{product_id}"
            
            if self.intelligence_system.redis_client:
                recent_changes = await self.intelligence_system.redis_client.lrange(change_key, 0, 50)
                
                if recent_changes:
                    changes_data = []
                    for change_json in recent_changes:
                        try:
                            change = json.loads(change_json)
                            changes_data.append(change)
                        except:
                            continue
                    
                    # Analizar patrones con la inteligencia del sistema
                    if changes_data:
                        analysis = await self.intelligence_system._update_volatility_metrics(
                            retailer, product_id, changes_data[0]  # √öltimo cambio
                        )
                        
                        # Obtener m√©tricas de volatilidad
                        cache_key = f"{retailer}:{product_id}"
                        if cache_key in self.intelligence_system.volatility_cache:
                            metrics = self.intelligence_system.volatility_cache[cache_key]
                            return {
                                'volatility_score': metrics.volatility_score,
                                'changes_24h': metrics.price_changes_24h,
                                'changes_7d': metrics.price_changes_7d,
                                'avg_change_magnitude': metrics.avg_change_magnitude,
                                'peak_hours': metrics.peak_hours,
                                'next_change_probability': metrics.next_change_probability,
                                'optimal_check_frequency_min': metrics.optimal_check_frequency,
                                'data_points': len(changes_data)
                            }
            
            # Sin datos - retornar an√°lisis por defecto
            return {
                'volatility_score': 0.5,
                'changes_24h': 0,
                'changes_7d': 0,
                'avg_change_magnitude': 0.0,
                'peak_hours': [],
                'next_change_probability': 0.3,
                'optimal_check_frequency_min': 60,
                'data_points': 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error analizando volatilidad: {e}")
            return {'volatility_score': 0.5, 'error': str(e)}
    
    async def _get_frequency_recommendation(self, retailer: str, product_id: str,
                                          volatility_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """‚ö° Obtener recomendaci√≥n de frecuencia de scraping"""
        try:
            if self.frequency_optimizer:
                # Simular datos de rendimiento actuales
                current_performance = {
                    'detection_accuracy': 0.85,
                    'success_rate': 0.90,
                    'avg_response_time': 1.2
                }
                
                # Simular cambios recientes basados en volatilidad
                recent_changes = []
                volatility_score = volatility_analysis.get('volatility_score', 0.5)
                changes_24h = volatility_analysis.get('changes_24h', 0)
                
                if changes_24h > 0:
                    for i in range(min(changes_24h, 10)):
                        recent_changes.append({
                            'timestamp': (datetime.now() - timedelta(hours=i*2)).isoformat(),
                            'change_percentage': volatility_score * 10,  # Simular magnitud
                            'hour': (datetime.now().hour - i*2) % 24
                        })
                
                # Obtener perfil optimizado
                optimized_profile = await self.frequency_optimizer.optimize_product_frequency(
                    retailer, product_id, recent_changes, current_performance
                )
                
                return {
                    'base_frequency_minutes': optimized_profile.base_frequency_minutes,
                    'peak_frequency_minutes': optimized_profile.peak_frequency_minutes,
                    'off_peak_frequency_minutes': optimized_profile.off_peak_frequency_minutes,
                    'peak_hours': optimized_profile.peak_hours,
                    'blackout_hours': optimized_profile.blackout_hours,
                    'commercial_value_score': optimized_profile.commercial_value_score,
                    'resource_cost_score': optimized_profile.resource_cost_score,
                    'savings_pct': max(0, (60 - optimized_profile.base_frequency_minutes) / 60 * 100)
                }
            
            # Fallback si no hay optimizador
            return {
                'base_frequency_minutes': 60,
                'peak_frequency_minutes': 30,
                'off_peak_frequency_minutes': 120,
                'peak_hours': [9, 10, 18, 19, 20],
                'blackout_hours': [],
                'savings_pct': 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error obteniendo recomendaci√≥n de frecuencia: {e}")
            return {'base_frequency_minutes': 60, 'error': str(e)}
    
    async def _assess_commercial_value(self, retailer: str, product_id: str,
                                     category: str = None) -> Dict[str, Any]:
        """üí∞ Evaluar valor comercial del producto"""
        try:
            base_score = 0.5
            
            # Factor por categor√≠a
            category_scores = {
                'celulares': 1.0,
                'smartphones': 1.0,
                'computadores': 0.9,
                'tablets': 0.8,
                'televisores': 0.7,
                'electrodomesticos': 0.6
            }
            
            category_factor = 0.8  # Por defecto
            if category:
                category_factor = category_scores.get(category.lower(), 0.8)
            
            # Factor por retailer
            retailer_factors = {
                'falabella': 1.0,
                'ripley': 0.95,
                'paris': 0.9,
                'hites': 0.8,
                'abcdin': 0.75
            }
            
            retailer_factor = retailer_factors.get(retailer.lower(), 0.8)
            
            # Score final
            final_score = base_score * category_factor * retailer_factor
            
            return {
                'score': min(1.0, final_score),
                'category_factor': category_factor,
                'retailer_factor': retailer_factor,
                'priority_level': 'high' if final_score > 0.8 else 'medium' if final_score > 0.6 else 'low'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error evaluando valor comercial: {e}")
            return {'score': 0.5, 'error': str(e)}
    
    def _recommend_tier(self, volatility_analysis: Dict[str, Any],
                       commercial_value: Dict[str, Any]) -> Dict[str, Any]:
        """üéöÔ∏è Recomendar tier apropiado"""
        try:
            volatility_score = volatility_analysis.get('volatility_score', 0.5)
            value_score = commercial_value.get('score', 0.5)
            
            # Combinar factores para determinar tier
            combined_score = (volatility_score * 0.6) + (value_score * 0.4)
            
            if combined_score >= 0.8:
                tier = 'critical'
                frequency_hours = 2
            elif combined_score >= 0.6:
                tier = 'important'
                frequency_hours = 6
            else:
                tier = 'tracking'
                frequency_hours = 24
            
            return {
                'recommended_tier': tier,
                'tier_frequency_hours': frequency_hours,
                'combined_score': combined_score,
                'reasoning': f'Volatilidad: {volatility_score:.2f}, Valor: {value_score:.2f}',
                'confidence': min(1.0, combined_score)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error recomendando tier: {e}")
            return {'recommended_tier': 'tracking', 'error': str(e)}
    
    def _calculate_analysis_confidence(self, volatility_analysis: Dict[str, Any],
                                     commercial_value: Dict[str, Any]) -> float:
        """üìä Calcular confianza del an√°lisis"""
        try:
            # Factores de confianza
            data_points = volatility_analysis.get('data_points', 0)
            data_confidence = min(1.0, data_points / 20.0)  # M√°xima confianza con 20+ puntos
            
            volatility_confidence = 1.0 - abs(0.5 - volatility_analysis.get('volatility_score', 0.5))
            value_confidence = commercial_value.get('score', 0.5)
            
            # Confianza combinada
            overall_confidence = (data_confidence * 0.4) + (volatility_confidence * 0.3) + (value_confidence * 0.3)
            
            return min(1.0, overall_confidence)
            
        except Exception as e:
            logger.error(f"‚ùå Error calculando confianza: {e}")
            return 0.5
    
    async def _record_decision(self, decision_type: str, target: str, action: str,
                             confidence: float, reasoning: str, expected_impact: Dict[str, Any]):
        """üìù Registrar decisi√≥n tomada por la inteligencia"""
        try:
            decision = IntelligenceDecision(
                decision_type=decision_type,
                target=target,
                action=action,
                confidence=confidence,
                reasoning=reasoning,
                expected_impact=expected_impact
            )
            
            self.decision_history.append(decision)
            
            # Mantener historial limitado
            if len(self.decision_history) > self.config['decision_history_limit']:
                self.decision_history = self.decision_history[-500:]
            
            # Ejecutar callbacks de decisi√≥n
            for callback in self.decision_callbacks:
                try:
                    await callback(decision)
                except Exception as e:
                    logger.error(f"‚ùå Error en callback de decisi√≥n: {e}")
            
        except Exception as e:
            logger.error(f"‚ùå Error registrando decisi√≥n: {e}")
    
    async def _metrics_update_loop(self):
        """üìä Loop de actualizaci√≥n de m√©tricas"""
        while self.running:
            try:
                await self._update_consolidated_metrics()
                await asyncio.sleep(self.config['metrics_update_interval_seconds'])
            except Exception as e:
                logger.error(f"‚ùå Error actualizando m√©tricas: {e}")
                await asyncio.sleep(30)
    
    async def _update_consolidated_metrics(self):
        """üìà Actualizar m√©tricas consolidadas"""
        try:
            # M√©tricas del cache
            if self.cache_manager:
                cache_metrics = await self.cache_manager.get_metrics()
                self.metrics.cache_hit_rate = cache_metrics['performance']['overall_hit_rate']
                self.metrics.cache_memory_usage_mb = cache_metrics['l1_status']['size_mb']
                self.metrics.cache_predictions_accuracy = cache_metrics['performance']['prediction_accuracy']
            
            # M√©tricas del optimizador
            if self.frequency_optimizer:
                optimizer_metrics = await self.frequency_optimizer.get_system_metrics()
                self.metrics.profiles_optimized = optimizer_metrics['profiles_summary']['optimization_count']
                self.metrics.resource_savings_pct = optimizer_metrics.get('resource_savings_pct', 0.0)
            
            # M√©tricas de inteligencia
            if self.intelligence_system:
                self.metrics.products_analyzed = len(self.intelligence_system.volatility_cache)
                self.metrics.high_volatility_products = sum(
                    1 for metrics in self.intelligence_system.volatility_cache.values()
                    if metrics.volatility_score > 0.7
                )
            
            # Calcular scores generales
            self.metrics.intelligence_score = self._calculate_intelligence_score()
            self.metrics.system_uptime_hours = (datetime.now() - self.start_time).total_seconds() / 3600
            self.metrics.last_updated = datetime.now()
            
        except Exception as e:
            logger.error(f"‚ùå Error actualizando m√©tricas consolidadas: {e}")
    
    def _calculate_intelligence_score(self) -> float:
        """üß† Calcular score general de inteligencia"""
        try:
            # Factores del score
            cache_factor = self.metrics.cache_hit_rate
            success_factor = self.metrics.successful_operations / max(1, self.metrics.total_operations)
            confidence_factor = sum(d.confidence for d in self.decision_history[-50:]) / max(1, len(self.decision_history[-50:]))
            
            # Score combinado
            intelligence_score = (cache_factor * 0.3) + (success_factor * 0.4) + (confidence_factor * 0.3)
            
            return min(1.0, intelligence_score)
            
        except Exception as e:
            logger.error(f"‚ùå Error calculando intelligence score: {e}")
            return 0.5
    
    async def _intelligent_decision_loop(self):
        """üß† Loop de toma de decisiones inteligentes"""
        while self.running:
            try:
                await self._make_intelligent_decisions()
                await asyncio.sleep(self.config['decision_making_interval_seconds'])
            except Exception as e:
                logger.error(f"‚ùå Error en loop de decisiones: {e}")
                await asyncio.sleep(60)
    
    async def _make_intelligent_decisions(self):
        """üéØ Tomar decisiones inteligentes basadas en datos"""
        try:
            # Analizar estado actual del sistema
            current_score = self.metrics.intelligence_score
            
            # Decisi√≥n de optimizaci√≥n de emergencia
            if current_score < self.config['emergency_optimization_threshold']:
                await self._record_decision(
                    decision_type='emergency_optimization',
                    target='system',
                    action='trigger_emergency_optimization',
                    confidence=1.0,
                    reasoning=f'Score bajo detectado: {current_score:.2f}',
                    expected_impact={'performance_improvement': 0.2}
                )
                
                # Ejecutar optimizaci√≥n de emergencia
                if self.frequency_optimizer:
                    # En un sistema real, aqu√≠ se ejecutar√≠a la optimizaci√≥n
                    logger.warning("‚ö†Ô∏è Optimizaci√≥n de emergencia necesaria")
            
            # Decisiones de cache inteligente
            if self.metrics.cache_hit_rate < 0.6:
                await self._record_decision(
                    decision_type='cache_optimization',
                    target='cache_system', 
                    action='increase_preloading',
                    confidence=0.8,
                    reasoning=f'Hit rate bajo: {self.metrics.cache_hit_rate:.2f}',
                    expected_impact={'cache_improvement': 0.15}
                )
            
            # M√°s decisiones inteligentes se pueden agregar aqu√≠
            
        except Exception as e:
            logger.error(f"‚ùå Error tomando decisiones inteligentes: {e}")
    
    async def _health_monitoring_loop(self):
        """üè• Loop de monitoreo de salud"""
        while self.running:
            try:
                await self._perform_health_check()
                await asyncio.sleep(self.config['health_check_interval_seconds'])
            except Exception as e:
                logger.error(f"‚ùå Error en monitoreo de salud: {e}")
                await asyncio.sleep(120)
    
    async def _perform_health_check(self):
        """üè• Realizar verificaci√≥n de salud completa"""
        try:
            health_issues = []
            component_health = {}
            
            # Check Intelligence System
            if self.intelligence_system and self.intelligence_system.redis_client:
                try:
                    await self.intelligence_system.redis_client.ping()
                    component_health['intelligence_system'] = 'healthy'
                except Exception as e:
                    component_health['intelligence_system'] = f'unhealthy: {e}'
                    health_issues.append('intelligence_system')
            
            # Check Cache Manager
            if self.cache_manager and self.cache_manager.redis_client:
                try:
                    await self.cache_manager.redis_client.ping()
                    component_health['cache_manager'] = 'healthy'
                except Exception as e:
                    component_health['cache_manager'] = f'unhealthy: {e}'
                    health_issues.append('cache_manager')
            
            # Check Frequency Optimizer
            if self.frequency_optimizer:
                component_health['frequency_optimizer'] = 'healthy' if self.frequency_optimizer.running else 'stopped'
                if not self.frequency_optimizer.running:
                    health_issues.append('frequency_optimizer')
            
            # Determinar salud general
            if not health_issues:
                overall_health = 'healthy'
                self.health_status['consecutive_failures'] = 0
            else:
                overall_health = 'degraded' if len(health_issues) < len(component_health) else 'unhealthy'
                self.health_status['consecutive_failures'] += 1
            
            # Actualizar estado de salud
            self.health_status.update({
                'overall_health': overall_health,
                'component_health': component_health,
                'last_health_check': datetime.now(),
                'health_issues': health_issues
            })
            
            # Auto-recovery si es necesario
            if (self.config['auto_recovery_enabled'] and 
                self.health_status['consecutive_failures'] >= self.config['auto_recovery_threshold']):
                
                logger.warning("‚ö†Ô∏è Iniciando auto-recovery del sistema")
                await self._attempt_auto_recovery()
            
        except Exception as e:
            logger.error(f"‚ùå Error en verificaci√≥n de salud: {e}")
    
    async def _attempt_auto_recovery(self):
        """üîß Intentar recuperaci√≥n autom√°tica del sistema"""
        try:
            logger.info("üîß Intentando recuperaci√≥n autom√°tica...")
            
            recovery_actions = 0
            
            # Reconectar sistemas con problemas
            if 'intelligence_system' in self.health_status.get('health_issues', []):
                try:
                    await self.intelligence_system.initialize()
                    recovery_actions += 1
                except:
                    pass
            
            if 'cache_manager' in self.health_status.get('health_issues', []):
                try:
                    await self.cache_manager.initialize()
                    recovery_actions += 1
                except:
                    pass
            
            if recovery_actions > 0:
                logger.info(f"üîß Auto-recovery completado: {recovery_actions} acciones")
                self.health_status['consecutive_failures'] = 0
            else:
                logger.error("‚ùå Auto-recovery fall√≥")
                
        except Exception as e:
            logger.error(f"‚ùå Error en auto-recovery: {e}")
    
    async def get_intelligence_dashboard(self) -> Dict[str, Any]:
        """üìä Obtener dashboard completo de inteligencia"""
        try:
            dashboard = {
                'timestamp': datetime.now().isoformat(),
                'system_status': {
                    'initialized': self.initialized,
                    'running': self.running,
                    'uptime_hours': self.metrics.system_uptime_hours,
                    'health': self.health_status['overall_health']
                },
                'intelligence_metrics': asdict(self.metrics),
                'recent_decisions': [asdict(d) for d in self.decision_history[-10:]],
                'component_status': {
                    'intelligence_system': bool(self.intelligence_system),
                    'cache_manager': bool(self.cache_manager),
                    'frequency_optimizer': bool(self.frequency_optimizer)
                },
                'performance_summary': {
                    'intelligence_score': self.metrics.intelligence_score,
                    'cache_hit_rate': self.metrics.cache_hit_rate,
                    'success_rate': self.metrics.successful_operations / max(1, self.metrics.total_operations),
                    'avg_confidence': sum(d.confidence for d in self.decision_history[-50:]) / max(1, len(self.decision_history[-50:]))
                }
            }
            
            # Agregar m√©tricas espec√≠ficas de componentes si est√°n disponibles
            if self.cache_manager:
                cache_metrics = await self.cache_manager.get_metrics()
                dashboard['cache_details'] = cache_metrics
            
            if self.frequency_optimizer:
                optimizer_metrics = await self.frequency_optimizer.get_system_metrics()
                dashboard['optimizer_details'] = optimizer_metrics
            
            return dashboard
            
        except Exception as e:
            logger.error(f"‚ùå Error generando dashboard: {e}")
            return {'error': str(e), 'timestamp': datetime.now().isoformat()}
    
    def register_scraping_callback(self, callback: Callable):
        """üîó Registrar callback para eventos de scraping"""
        self.scraping_callbacks.append(callback)
    
    def register_decision_callback(self, callback: Callable):
        """üîó Registrar callback para decisiones de inteligencia"""
        self.decision_callbacks.append(callback)
    
    async def _emergency_shutdown(self):
        """üö® Shutdown de emergencia"""
        logger.error("üö® Iniciando shutdown de emergencia...")
        self.running = False
        await self.shutdown()
    
    async def shutdown(self):
        """üõë Shutdown graceful de todos los sistemas"""
        logger.info("üõë Deteniendo Master Intelligence Integrator...")
        
        self.running = False
        
        # Cancelar tasks de fondo
        for task in self.background_tasks:
            task.cancel()
        
        # Esperar que terminen las tasks
        if self.background_tasks:
            await asyncio.gather(*self.background_tasks, return_exceptions=True)
        
        # Shutdown de componentes
        if self.frequency_optimizer:
            await self.frequency_optimizer.shutdown()
        
        if self.cache_manager:
            await self.cache_manager.shutdown()
        
        # Intelligence system se maneja autom√°ticamente
        
        self.initialized = False
        
        logger.info("‚úÖ Master Intelligence Integrator detenido correctamente")


# Funci√≥n para crear e inicializar el integrador
async def create_intelligence_integrator(redis_url: str = "redis://localhost:6379/0",
                                       cache_memory_mb: int = 200) -> MasterIntelligenceIntegrator:
    """üè≠ Factory function para crear e inicializar el integrador"""
    integrator = MasterIntelligenceIntegrator(redis_url, cache_memory_mb)
    await integrator.initialize()
    return integrator


async def main():
    """üß™ Funci√≥n de prueba completa"""
    logging.basicConfig(level=logging.INFO)
    
    try:
        # Crear integrador
        integrator = await create_intelligence_integrator()
        
        # Probar an√°lisis de scraping
        analysis = await integrator.analyze_scraping_request(
            retailer='falabella',
            product_id='CL-SAMS-GALAXY-256GB-FAL-001', 
            category='celulares'
        )
        
        logger.info(f"üéØ An√°lisis resultado: {analysis}")
        
        # Obtener dashboard
        dashboard = await integrator.get_intelligence_dashboard()
        logger.info(f"üìä Dashboard: {dashboard}")
        
        # Esperar un poco para ver el sistema funcionando
        await asyncio.sleep(10)
        
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Interrumpido por usuario")
    finally:
        if 'integrator' in locals():
            await integrator.shutdown()


if __name__ == "__main__":
    asyncio.run(main())