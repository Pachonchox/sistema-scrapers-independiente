#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ðŸ•·ï¸ Base Scraper v5 - Clase Base con IntegraciÃ³n ML Completa
==========================================================

Scraper base profesional y robusto que sirve como fundaciÃ³n para todos
los scrapers especÃ­ficos por retailer. Incluye integraciÃ³n ML completa,
manejo avanzado de errores, optimizaciÃ³n automÃ¡tica y testing integrado.

Features:
- ðŸ¤– IntegraciÃ³n ML para detecciÃ³n de fallos y optimizaciÃ³n
- ðŸŒ GestiÃ³n inteligente de proxies y rate limiting
- ðŸ”„ Auto-recovery y circuit breakers integrados
- ðŸ“Š ETL optimizado con reducciÃ³n de campos
- ðŸ§ª Testing y debugging automÃ¡tico
- ðŸ“ˆ MÃ©tricas de performance en tiempo real
- ðŸ›¡ï¸ ProtecciÃ³n anti-detecciÃ³n avanzada

Author: Portable Orchestrator Team
Version: 5.0.0
"""

import sys
import os
import io
import asyncio
import logging
import json
import time
import hashlib
import random
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from dataclasses import dataclass, field, asdict
from pathlib import Path
from abc import ABC, abstractmethod
from urllib.parse import urljoin, urlparse, quote
import traceback
import re

# Forzar soporte UTF-8 y emojis
if sys.platform == 'win32':
    if sys.stdout.encoding != 'utf-8':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    if sys.stderr.encoding != 'utf-8':
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Imports del sistema
try:
    from playwright.async_api import Page, Browser, BrowserContext, Route, Request, async_playwright
    from bs4 import BeautifulSoup, Tag
except ImportError:
    Page = Browser = BrowserContext = Route = Request = None
    BeautifulSoup = Tag = None

# Imports internos
from .exceptions import *
from .field_mapper import ETLFieldMapper

# Importar sistema de respaldo Parquet
try:
    import sys
    sys.path.append(str(Path(__file__).parent.parent.parent))
    from core.parquet_backup_system import save_scraper_backup
    PARQUET_BACKUP_AVAILABLE = True
    logger.info("ðŸ“¦ Sistema de respaldo Parquet disponible")
except ImportError:
    PARQUET_BACKUP_AVAILABLE = False
    logger.warning("âš ï¸ Sistema de respaldo Parquet no disponible")

# Configurar logging con emojis
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ScrapingResult:
    """ðŸ“Š Resultado de operaciÃ³n de scraping (v5 unificado)"""
    success: bool
    products: List[Any] = field(default_factory=list)
    total_found: int = 0
    execution_time: float = 0.0
    session_id: str = ""
    source_url: str = ""
    retailer: str = ""
    category: str = ""
    error_message: Optional[str] = None
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    performance_metrics: Dict[str, float] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    
    def add_error(self, error: str) -> None:
        """âž• Agregar error al resultado"""
        self.errors.append(error)
        self.success = False
    
    def add_warning(self, warning: str) -> None:
        """âš ï¸ Agregar warning al resultado"""
        self.warnings.append(warning)
    
    def get_product_count(self) -> int:
        """ðŸ“Š Obtener cantidad de productos"""
        return len(self.products)

@dataclass
class RetailerSelectors:
    """ðŸŽ¯ Selectores CSS especÃ­ficos por retailer"""
    product_cards: List[str] = field(default_factory=list)
    product_name: List[str] = field(default_factory=list)
    price_normal: List[str] = field(default_factory=list)
    price_offer: List[str] = field(default_factory=list)
    price_card: List[str] = field(default_factory=list)
    brand: List[str] = field(default_factory=list)
    sku: List[str] = field(default_factory=list)
    rating: List[str] = field(default_factory=list)
    reviews_count: List[str] = field(default_factory=list)
    availability: List[str] = field(default_factory=list)
    image_url: List[str] = field(default_factory=list)
    product_url: List[str] = field(default_factory=list)
    
    # Selectores de pÃ¡gina
    next_page: List[str] = field(default_factory=list)
    load_more: List[str] = field(default_factory=list)
    pagination: List[str] = field(default_factory=list)
    
    # Selectores de detecciÃ³n de bloqueo
    blocked_indicators: List[str] = field(default_factory=list)
    captcha_indicators: List[str] = field(default_factory=list)

@dataclass
class ScrapingConfig:
    """âš™ï¸ ConfiguraciÃ³n de scraping para un retailer"""
    retailer: str
    base_url: str
    selectors: RetailerSelectors
    rate_limit: float = 1.0  # requests por segundo
    timeout: int = 30
    max_retries: int = 3
    use_proxy: bool = False
    headless: bool = True
    user_agents: List[str] = field(default_factory=list)
    custom_headers: Dict[str, str] = field(default_factory=dict)
    blocked_detection: bool = True
    auto_scroll: bool = True
    wait_for_load: bool = True
    screenshot_on_error: bool = True
    
    def __post_init__(self):
        if not self.user_agents:
            self.user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'
            ]

@dataclass
class ProductData:
    """ðŸ“¦ RepresentaciÃ³n de producto estÃ¡ndar v5"""
    title: str
    current_price: float = 0.0
    original_price: float = 0.0
    discount_percentage: int = 0
    card_price: float = 0.0
    currency: str = "CLP"
    availability: str = ""
    product_url: str = ""
    image_urls: List[str] = field(default_factory=list)
    brand: str = ""
    sku: str = ""
    rating: float = 0.0
    reviews_count: int = 0
    retailer: str = ""
    extraction_timestamp: datetime = field(default_factory=datetime.now)
    category: str = ""
    additional_info: Dict[str, Any] = field(default_factory=dict)

    def to_record(self) -> Dict[str, Any]:
        """Convierte a dict con campos alineados al contrato Excel y Master"""
        # Precios numÃ©ricos para Master
        precio_normal_num = int(self.original_price or 0)
        # Asumimos current_price como precio de oferta efectivo si aplica
        precio_oferta_num = int(self.current_price or 0)
        precio_tarjeta_num = int(self.card_price or 0)

        return {
            'nombre': self.title,
            'marca': self.brand,
            'sku': self.sku,
            'categoria': self.category,
            'retailer': self.retailer,
            'link': self.product_url,
            'precio_normal_num': precio_normal_num,
            'precio_oferta_num': precio_oferta_num,
            'precio_tarjeta_num': precio_tarjeta_num,
            'rating': self.rating,
            'reviews_count': self.reviews_count,
            # Opcionales comunes (mantener nombres esperados por el proyecto)
            'storage': self.additional_info.get('storage', self.additional_info.get('memoria', '')),
            'ram': self.additional_info.get('ram', ''),
            'screen': self.additional_info.get('screen', self.additional_info.get('screen_size', '')),
        }


class BaseScraperV5(ABC):
    """
    ðŸ•·ï¸ Scraper Base v5 - Clase Base Profesional con ML
    
    Scraper base robusto que proporciona toda la funcionalidad comÃºn:
    
    ðŸ¤– **INTEGRACIÃ“N ML:**
    - DetecciÃ³n automÃ¡tica de fallos con anÃ¡lisis ML
    - OptimizaciÃ³n de selectores basada en Ã©xito histÃ³rico
    - PredicciÃ³n de bloqueos y auto-recovery
    - AnÃ¡lisis de patrones de performance
    
    ðŸ›¡ï¸ **PROTECCIÃ“N ANTI-DETECCIÃ“N:**
    - RotaciÃ³n automÃ¡tica de user agents
    - Manejo inteligente de proxies
    - Patrones de navegaciÃ³n humanizados
    - DetecciÃ³n de captchas y bloqueos
    
    ðŸ“Š **ETL OPTIMIZADO:**
    - ReducciÃ³n inteligente de campos
    - NormalizaciÃ³n automÃ¡tica de datos
    - ValidaciÃ³n de calidad de datos
    - Formato de salida optimizado
    
    ðŸ§ª **TESTING INTEGRADO:**
    - ValidaciÃ³n automÃ¡tica de selectores
    - Capturas de pantalla en errores
    - AnÃ¡lisis HTML para debugging
    - MÃ©tricas de performance detalladas
    """
    
    def __init__(self, config: Union[ScrapingConfig, str]):
        """
        ðŸš€ Inicializar Base Scraper v5
        
        Args:
            config: ConfiguraciÃ³n especÃ­fica del retailer
        """
        # Permitir inicializaciÃ³n con nombre de retailer (compatibilidad con scrapers v5 existentes)
        if isinstance(config, str):
            self.retailer = config
            # ConfiguraciÃ³n mÃ­nima por defecto; selectores se definen en cada scraper
            self.config = ScrapingConfig(
                retailer=config,
                base_url="",
                selectors=RetailerSelectors(),
                rate_limit=1.0,
                timeout=30,
                max_retries=3,
                use_proxy=False,
                headless=True,
            )
        else:
            self.config = config
            self.retailer = config.retailer
        
        # Logger por instancia
        self.logger = logging.getLogger(f"{__name__}.{self.retailer}")

        # Componentes ML
        self.ml_failure_detector = None
        self.proxy_manager = None
        self.field_mapper = ETLFieldMapper()
        
        # Estado del scraper
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.context: Optional[BrowserContext] = None
        self.current_proxy: Optional[str] = None
        self.session_id = hashlib.md5(
            f"{self.retailer}_{datetime.now().isoformat()}".encode()
        ).hexdigest()[:8]
        
        # MÃ©tricas y performance
        self.performance_metrics: Dict[str, float] = {}
        self.request_count = 0
        self.successful_requests = 0
        self.failed_requests = 0
        self.start_time: Optional[datetime] = None
        self.total_products_scraped = 0
        
        # Rate limiting
        self.last_request_time = 0.0
        self.request_times: List[float] = []
        
        # Estado de recuperaciÃ³n
        self.consecutive_failures = 0
        self.last_failure_time: Optional[datetime] = None
        self.circuit_breaker_open = False
        
        # Directorio de logs y screenshots
        self.logs_dir = Path(f"logs/scrapers/{self.retailer}")
        self.logs_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ðŸ•·ï¸ BaseScraperV5 inicializado para {self.retailer.upper()}")
        logger.info(f"ðŸ“ Session ID: {self.session_id}")
    
    async def __aenter__(self):
        """ðŸš€ Context manager para inicializaciÃ³n automÃ¡tica"""
        await self.initialize()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ðŸ”š Context manager para cleanup automÃ¡tico"""
        await self.cleanup()
    
    async def initialize(self) -> bool:
        """
        ðŸš€ InicializaciÃ³n completa del scraper
        
        Returns:
            bool: True si la inicializaciÃ³n fue exitosa
        """
        try:
            logger.info(f"ðŸš€ Inicializando scraper para {self.retailer.upper()}...")
            self.start_time = datetime.now()
            
            # 1. Inicializar componentes ML
            await self._initialize_ml_components()
            
            # 2. Configurar browser
            if not await self._setup_browser():
                raise ScraperV5Exception("Error configurando browser")
            
            # 3. Configurar proxy si es necesario
            if self.config.use_proxy:
                await self._setup_proxy()
            
            # 4. Validar configuraciÃ³n
            await self._validate_configuration()
            
            logger.info(f"âœ… Scraper {self.retailer.upper()} inicializado correctamente")
            return True
            
        except Exception as e:
            logger.error(f"ðŸ’¥ Error inicializando scraper {self.retailer}: {str(e)}")
            await self.cleanup()
            return False
    
    async def cleanup(self) -> None:
        """
        ðŸ§¹ Limpieza de recursos
        """
        logger.info(f"ðŸ§¹ Limpiando recursos de {self.retailer.upper()}...")
        
        try:
            # Cerrar pÃ¡gina y context
            if self.page:
                await self.page.close()
                self.page = None
            
            if self.context:
                await self.context.close()
                self.context = None
            
            # No cerrar browser aquÃ­, se maneja externamente
            
            # Guardar mÃ©tricas finales
            await self._save_session_metrics()
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error en cleanup: {str(e)}")
        
        logger.info(f"âœ… Cleanup completado para {self.retailer.upper()}")
    
    @abstractmethod
    async def scrape_category(self, category_url: str, 
                            max_pages: Optional[int] = None) -> ScrapingResult:
        """
        ðŸŽ¯ MÃ©todo abstracto para scraping de categorÃ­a
        
        Debe ser implementado por cada scraper especÃ­fico.
        
        Args:
            category_url: URL de la categorÃ­a a scrapear
            max_pages: LÃ­mite mÃ¡ximo de pÃ¡ginas (None = sin lÃ­mite)
            
        Returns:
            ScrapingResult: Resultado del scraping
        """
        pass
    
    def get_selectors(self) -> RetailerSelectors:
        """ðŸŽ¯ Obtener selectores del retailer (por defecto, desde config)."""
        return self.config.selectors if hasattr(self, 'config') and self.config else RetailerSelectors()
    
    # ==========================================
    # MÃ‰TODOS DE SCRAPING COMUNES
    # ==========================================
    
    async def extract_products_from_page(self, url: str) -> ScrapingResult:
        """
        ðŸ“Š Extraer productos de una pÃ¡gina especÃ­fica
        
        Args:
            url: URL de la pÃ¡gina a scrapear
            
        Returns:
            ScrapingResult: Productos extraÃ­dos y mÃ©tricas
        """
        result = ScrapingResult()
        start_time = time.time()
        
        try:
            logger.info(f"ðŸ“„ Extrayendo productos de: {url}")
            
            # 1. Navegar a la pÃ¡gina
            navigation_result = await self._navigate_to_page(url)
            if not navigation_result:
                result.add_error("Error navegando a la pÃ¡gina")
                return result
            
            # 2. Detectar bloqueos
            if await self._detect_blocking():
                result.add_error("PÃ¡gina bloqueada o con captcha")
                await self._handle_blocking()
                return result
            
            # 3. Esperar carga completa
            await self._wait_for_page_load()
            
            # 4. Scroll para cargar contenido lazy
            if self.config.auto_scroll:
                await self._intelligent_scroll()
            
            # 5. Extraer productos
            products = await self._extract_products_with_ml()
            result.products = products
            result.success = len(products) > 0
            
            # 6. Validar calidad de datos
            await self._validate_extracted_data(result)
            
            # 7. Guardar respaldo en Parquet (datos crudos)
            if PARQUET_BACKUP_AVAILABLE and products:
                try:
                    category = self._extract_category_from_url(url)
                    backup_metadata = {
                        'session_id': result.session_id,
                        'source_urls': [url],
                        'execution_time': time.time() - start_time,
                        'success_rate': 1.0 if result.success else 0.0,
                        'errors': result.errors,
                        'warnings': result.warnings
                    }
                    
                    backup_result = save_scraper_backup(
                        retailer=self.retailer,
                        category=category,
                        products=[asdict(p) if hasattr(p, '__dict__') else p for p in products],
                        metadata=backup_metadata
                    )
                    
                    if backup_result.get('success'):
                        logger.info(f"ðŸ“¦ Respaldo Parquet guardado: {backup_result.get('file_path')}")
                        result.metadata['parquet_backup'] = backup_result
                    else:
                        logger.warning(f"âš ï¸ Error guardando respaldo Parquet: {backup_result.get('error')}")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Error creando respaldo Parquet: {e}")
            
            # 8. MÃ©tricas de performance
            duration = time.time() - start_time
            result.performance_metrics = {
                'extraction_time': duration,
                'products_per_second': len(products) / duration if duration > 0 else 0,
                'page_load_time': self.performance_metrics.get('page_load_time', 0),
                'success_rate': 1.0 if result.success else 0.0
            }
            
            logger.info(f"âœ… ExtraÃ­dos {len(products)} productos en {duration:.2f}s")
            
        except Exception as e:
            result.add_error(f"Error extrayendo productos: {str(e)}")
            logger.error(f"ðŸ’¥ Error en extracciÃ³n: {str(e)}")
            
            # Screenshot en error si estÃ¡ habilitado
            if self.config.screenshot_on_error:
                await self._capture_error_screenshot("extraction_error")
        
        return result
    
    def _extract_category_from_url(self, url: str) -> str:
        """Extraer categorÃ­a desde URL para respaldo Parquet"""
        try:
            # Extraer categorÃ­a de la URL
            url_parts = url.lower().split('/')
            
            # Buscar indicadores comunes de categorÃ­a
            category_indicators = [
                'celulares', 'smartphones', 'telefonos',
                'laptops', 'notebooks', 'computadores',
                'televisores', 'tv', 'smart-tv',
                'electrohogar', 'electrodomesticos',
                'gaming', 'juegos', 'consolas',
                'audio', 'parlantes', 'audifonos',
                'hogar', 'muebles', 'decoracion'
            ]
            
            for part in url_parts:
                for indicator in category_indicators:
                    if indicator in part:
                        return indicator
            
            # Fallback: usar el retailer como categorÃ­a
            return self.retailer if hasattr(self, 'retailer') else 'general'
            
        except Exception:
            return 'general'
    
    async def _navigate_to_page(self, url: str) -> bool:
        """
        ðŸ§­ NavegaciÃ³n inteligente a pÃ¡gina con retry y error handling
        
        Args:
            url: URL a la que navegar
            
        Returns:
            bool: True si la navegaciÃ³n fue exitosa
        """
        if not self.page:
            logger.error("âŒ PÃ¡gina no inicializada")
            return False
        
        try:
            # Rate limiting
            await self._respect_rate_limit()
            
            # Configurar headers
            await self._set_random_headers()
            
            # Navegar con timeout
            logger.debug(f"ðŸ§­ Navegando a: {url}")
            start_time = time.time()
            
            response = await self.page.goto(
                url,
                timeout=self.config.timeout * 1000,
                wait_until='networkidle'
            )
            
            load_time = time.time() - start_time
            self.performance_metrics['page_load_time'] = load_time
            
            # Verificar respuesta
            if response and response.status >= 400:
                if response.status == 403:
                    raise RetailerBlockedException(
                        retailer=self.retailer,
                        url=url,
                        detection_indicators=[f"HTTP {response.status}"]
                    )
                elif response.status >= 500:
                    raise NetworkException(
                        message=f"Error del servidor: HTTP {response.status}",
                        url=url,
                        status_code=response.status
                    )
            
            self.request_count += 1
            self.successful_requests += 1
            
            logger.debug(f"âœ… NavegaciÃ³n exitosa en {load_time:.2f}s")
            return True
            
        except TimeoutError:
            self.failed_requests += 1
            raise TimeoutException(
                message="Timeout navegando a pÃ¡gina",
                timeout_seconds=self.config.timeout,
                operation="navigation",
                context={'url': url}
            )
        except Exception as e:
            self.failed_requests += 1
            logger.error(f"ðŸ’¥ Error navegando a {url}: {str(e)}")
            return False
    
    async def _detect_blocking(self) -> bool:
        """
        ðŸ›¡ï¸ DetecciÃ³n inteligente de bloqueos y captchas
        
        Returns:
            bool: True si se detecta bloqueo
        """
        if not self.config.blocked_detection or not self.page:
            return False
        
        try:
            # Buscar indicadores de bloqueo en selectores
            blocked_selectors = self.config.selectors.blocked_indicators
            for selector in blocked_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        logger.warning(f"ðŸš« Bloqueo detectado con selector: {selector}")
                        return True
                except:
                    continue
            
            # Buscar captchas
            captcha_selectors = self.config.selectors.captcha_indicators
            for selector in captcha_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        logger.warning(f"ðŸ¤– Captcha detectado con selector: {selector}")
                        return True
                except:
                    continue
            
            # DetecciÃ³n por contenido de texto
            page_content = await self.page.content()
            blocking_keywords = [
                'blocked', 'captcha', 'verification', 'access denied',
                'not authorized', 'bot detection', 'human verification'
            ]
            
            content_lower = page_content.lower()
            for keyword in blocking_keywords:
                if keyword in content_lower:
                    logger.warning(f"ðŸš« Bloqueo detectado por keyword: {keyword}")
                    return True
            
            # DetecciÃ³n por tÃ­tulo de pÃ¡gina
            title = await self.page.title()
            if title and any(word in title.lower() for word in ['blocked', 'access', 'error']):
                logger.warning(f"ðŸš« Bloqueo detectado en tÃ­tulo: {title}")
                return True
            
            return False
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error en detecciÃ³n de bloqueo: {str(e)}")
            return False
    
    async def _handle_blocking(self) -> bool:
        """
        ðŸ”„ Manejo automÃ¡tico de bloqueos
        
        Returns:
            bool: True si se pudo resolver el bloqueo
        """
        logger.warning(f"ðŸš« Manejando bloqueo para {self.retailer.upper()}...")
        
        try:
            # Capturar screenshot del bloqueo
            await self._capture_error_screenshot("blocking_detected")
            
            # 1. Cambiar user agent
            await self._rotate_user_agent()
            
            # 2. Cambiar proxy si estÃ¡ disponible
            if self.config.use_proxy and self.proxy_manager:
                new_proxy = await self.proxy_manager.get_fresh_proxy(self.retailer)
                if new_proxy:
                    await self._change_proxy(new_proxy)
                    logger.info("ðŸŒ Proxy cambiado")
            
            # 3. Espera mÃ¡s larga
            wait_time = min(300, 30 * (self.consecutive_failures + 1))  # Max 5 minutos
            logger.info(f"â³ Esperando {wait_time}s antes de continuar...")
            await asyncio.sleep(wait_time)
            
            # 4. Reinicializar contexto del browser
            await self._reinitialize_browser_context()
            
            self.consecutive_failures += 1
            self.last_failure_time = datetime.now()
            
            # Circuit breaker si hay muchos fallos
            if self.consecutive_failures >= 5:
                self.circuit_breaker_open = True
                logger.error(f"âš¡ Circuit breaker abierto para {self.retailer}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"ðŸ’¥ Error manejando bloqueo: {str(e)}")
            return False
    
    async def _extract_products_with_ml(self) -> List[Dict[str, Any]]:
        """
        ðŸ¤– ExtracciÃ³n de productos con optimizaciÃ³n ML
        
        Returns:
            List[Dict]: Lista de productos extraÃ­dos
        """
        products = []
        
        try:
            if not self.page:
                return products
            
            # Obtener selectores optimizados por ML
            selectors = await self._get_optimized_selectors()
            
            # Buscar tarjetas de producto
            product_cards = await self._find_product_cards(selectors.product_cards)
            if not product_cards:
                logger.warning("âš ï¸ No se encontraron tarjetas de producto")
                return products
            
            logger.info(f"ðŸ” Encontradas {len(product_cards)} tarjetas de producto")
            
            # Extraer datos de cada tarjeta
            for i, card in enumerate(product_cards):
                try:
                    product_data = await self._extract_product_from_card(card, selectors)
                    
                    if product_data:
                        # Aplicar reducciÃ³n ETL
                        optimized_product = self.field_mapper.reduce_fields(
                            product_data, self.retailer
                        )
                        products.append(optimized_product)
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Error extrayendo producto {i+1}: {str(e)}")
                    continue
            
            logger.info(f"âœ… {len(products)} productos extraÃ­dos exitosamente")
            
        except Exception as e:
            logger.error(f"ðŸ’¥ Error en extracciÃ³n ML: {str(e)}")
        
        return products
    
    async def _find_product_cards(self, card_selectors: List[str]) -> List:
        """
        ðŸ” Encontrar tarjetas de producto usando selectores con fallback
        
        Args:
            card_selectors: Lista de selectores CSS para buscar
            
        Returns:
            List: Lista de elementos de tarjetas encontradas
        """
        for selector in card_selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                if elements:
                    logger.debug(f"âœ… Selector exitoso: {selector} ({len(elements)} elementos)")
                    return elements
                else:
                    logger.debug(f"âŒ Selector sin resultados: {selector}")
            except Exception as e:
                logger.debug(f"ðŸ’¥ Error con selector {selector}: {str(e)}")
                continue
        
        logger.warning("âš ï¸ NingÃºn selector de tarjetas funcionÃ³")
        return []
    
    async def _extract_product_from_card(self, card_element, 
                                       selectors: RetailerSelectors) -> Optional[Dict[str, Any]]:
        """
        ðŸ“Š Extraer datos de producto de una tarjeta
        
        Args:
            card_element: Elemento DOM de la tarjeta
            selectors: Selectores CSS para los campos
            
        Returns:
            Dict con datos del producto o None si falla
        """
        product = {}
        
        try:
            # Extraer cada campo usando selectores con fallback
            product['nombre'] = await self._extract_field(
                card_element, selectors.product_name, 'text'
            )
            
            product['precio_normal'] = await self._extract_price_field(
                card_element, selectors.price_normal
            )
            
            product['precio_oferta'] = await self._extract_price_field(
                card_element, selectors.price_offer
            )
            
            product['precio_tarjeta'] = await self._extract_price_field(
                card_element, selectors.price_card
            )
            
            product['marca'] = await self._extract_field(
                card_element, selectors.brand, 'text'
            )
            
            product['sku'] = await self._extract_field(
                card_element, selectors.sku, 'attribute', 'data-sku'
            )
            
            product['rating'] = await self._extract_numeric_field(
                card_element, selectors.rating
            )
            
            product['reviews_count'] = await self._extract_numeric_field(
                card_element, selectors.reviews_count
            )
            
            product['imagen_url'] = await self._extract_field(
                card_element, selectors.image_url, 'attribute', 'src'
            )
            
            product['link'] = await self._extract_field(
                card_element, selectors.product_url, 'attribute', 'href'
            )
            
            # Limpiar y normalizar datos
            product = self._clean_product_data(product)
            
            # Validar que tenga campos mÃ­nimos requeridos
            if not product.get('nombre') or not any([
                product.get('precio_normal'),
                product.get('precio_oferta'), 
                product.get('precio_tarjeta')
            ]):
                logger.debug("âŒ Producto sin datos mÃ­nimos requeridos")
                return None
            
            # Agregar metadatos
            product['retailer'] = self.retailer
            product['extracted_at'] = datetime.now().isoformat()
            product['session_id'] = self.session_id
            
            return product
            
        except Exception as e:
            logger.debug(f"ðŸ’¥ Error extrayendo producto: {str(e)}")
            return None
    
    # ==========================================
    # MÃ‰TODOS DE UTILIDAD Y HELPERS
    # ==========================================
    
    async def _extract_field(self, element, selectors: List[str], 
                            extraction_type: str = 'text',
                            attribute: Optional[str] = None) -> Optional[str]:
        """ðŸ” Extraer campo usando selectores con fallback"""
        for selector in selectors:
            try:
                field_element = await element.query_selector(selector)
                if not field_element:
                    continue
                
                if extraction_type == 'text':
                    value = await field_element.inner_text()
                elif extraction_type == 'attribute':
                    value = await field_element.get_attribute(attribute or 'value')
                else:
                    value = None
                
                if value and value.strip():
                    return value.strip()
                    
            except Exception:
                continue
        
        return None
    
    async def _extract_price_field(self, element, selectors: List[str]) -> Optional[float]:
        """ðŸ’° Extraer campo de precio con limpieza automÃ¡tica"""
        price_text = await self._extract_field(element, selectors, 'text')
        if not price_text:
            return None
        
        # Limpiar precio usando regex
        price_clean = re.sub(r'[^\d,.]', '', price_text)
        price_clean = price_clean.replace(',', '')
        
        try:
            return float(price_clean)
        except (ValueError, TypeError):
            return None
    
    async def _extract_numeric_field(self, element, selectors: List[str]) -> Optional[float]:
        """ðŸ”¢ Extraer campo numÃ©rico"""
        text = await self._extract_field(element, selectors, 'text')
        if not text:
            return None
        
        # Buscar primer nÃºmero en el texto
        match = re.search(r'(\d+(?:\.\d+)?)', text)
        if match:
            try:
                return float(match.group(1))
            except:
                pass
        
        return None
    
    def _clean_product_data(self, product: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ§¹ Limpiar y normalizar datos de producto"""
        cleaned = {}
        
        for key, value in product.items():
            if value is None:
                continue
            
            if isinstance(value, str):
                # Limpiar strings
                value = value.strip()
                value = re.sub(r'\s+', ' ', value)  # Normalizar espacios
                
                if not value:
                    continue
            
            cleaned[key] = value
        
        return cleaned
    
    # ==========================================
    # MÃ‰TODOS DE CONFIGURACIÃ“N Y SETUP
    # ==========================================
    
    async def _initialize_ml_components(self) -> None:
        """ðŸ¤– Inicializar componentes ML"""
        try:
            # Cargar detector de fallos ML (se implementarÃ¡ despuÃ©s)
            # self.ml_failure_detector = await self._load_failure_detector()
            
            # Cargar gestor de proxies ML (se implementarÃ¡ despuÃ©s)  
            # self.proxy_manager = await self._load_proxy_manager()
            
            logger.debug("ðŸ¤– Componentes ML inicializados")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error inicializando ML: {str(e)}")

    async def get_page(self) -> Optional[Page]:
        """Crear/obtener una pÃ¡gina lista para navegar.

        Lanza Chromium si no hay browser disponible y configura contexto/pÃ¡gina.
        """
        try:
            if not self.browser:
                # Lanzar navegador
                pw = await async_playwright().start()
                launch_kwargs = {
                    'headless': True,
                    'args': [
                        '--disable-blink-features=AutomationControlled',
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                    ]
                }
                try:
                    self.browser = await pw.chromium.launch(**launch_kwargs)
                except Exception:
                    # Fallback headless_shell si aplica
                    self.browser = await pw.chromium.launch(headless=True)

            if not await self._setup_browser():
                return None
            return self.page
        except Exception as e:
            logger.error(f"ðŸ’¥ Error creando pÃ¡gina: {e}")
            return None
    
    async def _setup_browser(self) -> bool:
        """ðŸŒ Configurar browser y context"""
        if not self.browser:
            # Intentar lanzar browser si no existe
            try:
                pw = await async_playwright().start()
                self.browser = await pw.chromium.launch(headless=True)
            except Exception as e:
                logger.error(f"âŒ Browser no disponible: {e}")
                return False
        
        try:
            # Crear contexto con configuraciÃ³n anti-detecciÃ³n
            context_options = {
                'viewport': {'width': 1920, 'height': 1080},
                'user_agent': random.choice(self.config.user_agents),
                'locale': 'es-CL',
                'timezone_id': 'America/Santiago',
                'geolocation': {'latitude': -33.4489, 'longitude': -70.6693},  # Santiago
                'permissions': ['geolocation']
            }
            
            # Agregar proxy si estÃ¡ configurado
            if self.current_proxy:
                context_options['proxy'] = {'server': self.current_proxy}
            
            self.context = await self.browser.new_context(**context_options)
            
            # Crear pÃ¡gina
            self.page = await self.context.new_page()
            
            # Configurar interceptores
            await self._setup_request_interceptors()
            
            logger.debug("ðŸŒ Browser configurado correctamente")
            return True
            
        except Exception as e:
            logger.error(f"ðŸ’¥ Error configurando browser: {str(e)}")
            return False
    
    async def _setup_request_interceptors(self) -> None:
        """ðŸ”— Configurar interceptores de requests"""
        if not self.page:
            return
        
        async def route_handler(route: Route, request: Request):
            # Bloquear recursos innecesarios para optimizar velocidad
            if request.resource_type in ['image', 'stylesheet', 'font', 'media']:
                await route.abort()
            else:
                await route.continue_()
        
        # Interceptar requests
        await self.page.route("**/*", route_handler)
    
    async def _setup_proxy(self) -> None:
        """ðŸŒ Configurar proxy si estÃ¡ disponible"""
        try:
            if self.proxy_manager:
                proxy = await self.proxy_manager.get_best_proxy(self.retailer)
                if proxy:
                    self.current_proxy = proxy
                    logger.info(f"ðŸŒ Proxy configurado: {proxy}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error configurando proxy: {str(e)}")
    
    async def _validate_configuration(self) -> None:
        """âœ… Validar configuraciÃ³n del scraper"""
        errors = []
        
        if not self.config.base_url:
            errors.append("base_url no configurado")
        
        if not self.config.selectors.product_cards:
            errors.append("Selectores de tarjetas de producto no configurados")
        
        if self.config.rate_limit <= 0:
            errors.append("rate_limit debe ser mayor a 0")
        
        if errors:
            raise ConfigurationException(
                config_key="scraper_config",
                context={'retailer': self.retailer, 'errors': errors}
            )
    
    # ==========================================
    # MÃ‰TODOS DE OPTIMIZACIÃ“N Y ML
    # ==========================================
    
    async def _get_optimized_selectors(self) -> RetailerSelectors:
        """ðŸŽ¯ Obtener selectores optimizados por ML"""
        # Por ahora retorna selectores base
        # En el futuro incluirÃ¡ optimizaciÃ³n ML basada en Ã©xito histÃ³rico
        return self.config.selectors
    
    async def _respect_rate_limit(self) -> None:
        """ðŸš¦ Respetar rate limiting"""
        if self.config.rate_limit <= 0:
            return
        
        min_interval = 1.0 / self.config.rate_limit
        current_time = time.time()
        
        if self.last_request_time > 0:
            time_since_last = current_time - self.last_request_time
            if time_since_last < min_interval:
                sleep_time = min_interval - time_since_last
                await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    async def _set_random_headers(self) -> None:
        """ðŸŽ² Configurar headers aleatorios"""
        if not self.page:
            return
        
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'es-CL,es;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # Agregar headers custom
        headers.update(self.config.custom_headers)
        
        await self.page.set_extra_http_headers(headers)
    
    async def _intelligent_scroll(self) -> None:
        """ðŸ“œ Scroll inteligente para cargar contenido lazy"""
        if not self.page:
            return
        
        try:
            # Obtener dimensiones
            viewport_height = await self.page.evaluate('window.innerHeight')
            total_height = await self.page.evaluate('document.body.scrollHeight')
            
            # Scroll gradual
            current_position = 0
            scroll_step = viewport_height // 3  # Scroll de 1/3 de viewport
            
            while current_position < total_height:
                # Scroll suave
                await self.page.evaluate(f'window.scrollTo(0, {current_position})')
                
                # Pausa para carga lazy
                await asyncio.sleep(random.uniform(0.5, 1.5))
                
                current_position += scroll_step
                
                # Recalcular altura (puede crecer con lazy loading)
                new_height = await self.page.evaluate('document.body.scrollHeight')
                if new_height > total_height:
                    total_height = new_height
            
            # Volver al top
            await self.page.evaluate('window.scrollTo(0, 0)')
            
        except Exception as e:
            logger.debug(f"âš ï¸ Error en scroll inteligente: {str(e)}")
    
    async def _wait_for_page_load(self) -> None:
        """â³ Esperar carga completa de pÃ¡gina"""
        if not self.page or not self.config.wait_for_load:
            return
        
        try:
            # Esperar que el DOM estÃ© listo
            await self.page.wait_for_load_state('domcontentloaded')
            
            # Esperar que no haya requests de red pendientes
            await self.page.wait_for_load_state('networkidle')
            
            # Pausa adicional para JavaScript asÃ­ncrono
            await asyncio.sleep(random.uniform(1, 3))
            
        except Exception as e:
            logger.debug(f"âš ï¸ Error esperando carga: {str(e)}")
    
    # ==========================================
    # MÃ‰TODOS DE RECOVERY Y ERROR HANDLING
    # ==========================================
    
    async def _rotate_user_agent(self) -> None:
        """ðŸŽ­ Rotar user agent"""
        if not self.page:
            return
        
        new_user_agent = random.choice(self.config.user_agents)
        await self.page.set_user_agent(new_user_agent)
        logger.debug(f"ðŸŽ­ User agent rotado")
    
    async def _change_proxy(self, new_proxy: str) -> None:
        """ðŸŒ Cambiar proxy reinicializando contexto"""
        try:
            # Cerrar contexto actual
            if self.context:
                await self.context.close()
            
            # Actualizar proxy
            self.current_proxy = new_proxy
            
            # Reinicializar contexto con nuevo proxy
            await self._setup_browser()
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error cambiando proxy: {str(e)}")
    
    async def _reinitialize_browser_context(self) -> None:
        """ðŸ”„ Reinicializar contexto del browser"""
        try:
            if self.context:
                await self.context.close()
            
            await self._setup_browser()
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error reinicializando contexto: {str(e)}")
    
    async def _capture_error_screenshot(self, error_type: str) -> Optional[str]:
        """ðŸ“¸ Capturar screenshot en caso de error"""
        if not self.config.screenshot_on_error or not self.page:
            return None
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.retailer}_{error_type}_{timestamp}.png"
            screenshot_path = self.logs_dir / "screenshots" / filename
            screenshot_path.parent.mkdir(exist_ok=True)
            
            await self.page.screenshot(path=str(screenshot_path), full_page=True)
            logger.info(f"ðŸ“¸ Screenshot capturado: {screenshot_path}")
            
            return str(screenshot_path)
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error capturando screenshot: {str(e)}")
            return None
    
    async def _validate_extracted_data(self, result: ScrapingResult) -> None:
        """âœ… Validar calidad de datos extraÃ­dos"""
        if not result.products:
            result.add_warning("No se extrajeron productos")
            return
        
        # Validaciones de calidad
        products_with_prices = len([
            p for p in result.products 
            if any([p.get('precio_normal'), p.get('precio_oferta'), p.get('precio_tarjeta')])
        ])
        
        price_coverage = products_with_prices / len(result.products)
        if price_coverage < 0.8:  # <80% con precios
            result.add_warning(f"Baja cobertura de precios: {price_coverage:.1%}")
        
        # Validar nombres de producto
        products_with_names = len([p for p in result.products if p.get('nombre')])
        name_coverage = products_with_names / len(result.products)
        if name_coverage < 0.9:  # <90% con nombres
            result.add_warning(f"Baja cobertura de nombres: {name_coverage:.1%}")
    
    async def _save_session_metrics(self) -> None:
        """ðŸ’¾ Guardar mÃ©tricas de sesiÃ³n"""
        try:
            if not self.start_time:
                return
            
            duration = datetime.now() - self.start_time
            
            metrics = {
                'session_id': self.session_id,
                'retailer': self.retailer,
                'start_time': self.start_time.isoformat(),
                'duration_seconds': duration.total_seconds(),
                'request_count': self.request_count,
                'successful_requests': self.successful_requests,
                'failed_requests': self.failed_requests,
                'total_products_scraped': self.total_products_scraped,
                'success_rate': self.successful_requests / max(1, self.request_count),
                'products_per_minute': self.total_products_scraped / max(1, duration.total_seconds() / 60),
                'consecutive_failures': self.consecutive_failures,
                'circuit_breaker_open': self.circuit_breaker_open,
                'performance_metrics': self.performance_metrics
            }
            
            # Guardar en archivo
            metrics_file = self.logs_dir / f"session_{self.session_id}_metrics.json"
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            
            logger.debug(f"ðŸ’¾ MÃ©tricas guardadas: {metrics_file}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Error guardando mÃ©tricas: {str(e)}")

# ==========================================
# FUNCIONES DE UTILIDAD
# ==========================================

def create_base_selectors() -> RetailerSelectors:
    """
    ðŸŽ¯ Crear selectores base comunes para retailers
    
    Returns:
        RetailerSelectors: Selectores CSS base
    """
    return RetailerSelectors(
        product_cards=[
            '.product-item', '.product-card', '.product', 
            '[data-product]', '.item', '.card'
        ],
        product_name=[
            '.product-name', '.name', '.title', 'h1', 'h2', 'h3',
            '[data-name]', '.product-title'
        ],
        price_normal=[
            '.price', '.normal-price', '.original-price', 
            '[data-price]', '.price-normal'
        ],
        price_offer=[
            '.offer-price', '.sale-price', '.discount-price',
            '.price-offer', '.special-price'
        ],
        price_card=[
            '.card-price', '.member-price', '.tc-price',
            '.price-card'
        ],
        brand=[
            '.brand', '.marca', '[data-brand]', '.brand-name'
        ],
        sku=[
            '[data-sku]', '[data-id]', '.sku', '.product-id'
        ],
        blocked_indicators=[
            '.blocked', '.access-denied', '.captcha',
            '#captcha', '.verification'
        ],
        captcha_indicators=[
            '.captcha', '#captcha', '.recaptcha', 
            '.verification', '.challenge'
        ]
    )
