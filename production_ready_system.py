# -*- coding: utf-8 -*-
"""
üöÄ Sistema Listo para Producci√≥n - Proxy Inteligente + Ahorro de Datos
=====================================================================

SISTEMA FINAL que combina:
‚úÖ 70% tr√°fico directo / 30% proxy inteligente
‚úÖ Detecci√≥n autom√°tica de bloqueos ‚Üí Switch a proxy  
‚úÖ 93.6% ahorro de datos en Falabella
‚úÖ 50.1% ahorro de datos en Ripley
‚úÖ Bloqueo de 33+ dominios no esenciales
‚úÖ Proxy Decodo integrado
‚úÖ Fallback autom√°tico
‚úÖ M√©tricas en tiempo real

LISTO PARA USO INMEDIATO EN PRODUCCI√ìN
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime
import json
from pathlib import Path

from smart_proxy_data_saver import SmartProxyDataSaver

# Configurar logging para producci√≥n
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler('production_scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("production_system")

class ProductionScrapingSystem:
    """Sistema de scraping listo para producci√≥n"""
    
    def __init__(self):
        self.systems = {}  # Cache de sistemas por retailer
        self.global_stats = {
            "sessions_created": 0,
            "total_requests": 0,
            "total_bytes_saved": 0,
            "proxy_usage_ratio": 0,
            "success_rate": 0
        }
        
        # Configuraci√≥n de retailers soportados
        self.supported_retailers = {
            "falabella": {
                "name": "Falabella",
                "priority": 1,
                "savings_percent": 93.6,
                "recommended": True,
                "urls": {
                    "smartphones": "https://www.falabella.com/falabella-cl/category/cat720161/Smartphones",
                    "computadores": "https://www.falabella.com/falabella-cl/category/cat40052/Computadores",
                    "tablets": "https://www.falabella.com/falabella-cl/category/cat1012/Tablets"
                }
            },
            "ripley": {
                "name": "Ripley", 
                "priority": 2,
                "savings_percent": 50.1,
                "recommended": True,
                "urls": {
                    "computacion": "https://simple.ripley.cl/tecno/computacion",
                    "smartphones": "https://simple.ripley.cl/tecno/celulares-smartphones",
                    "tablets": "https://simple.ripley.cl/tecno/tablets"
                }
            },
            "paris": {
                "name": "Paris",
                "priority": 3,
                "savings_percent": 20.0,
                "recommended": False,  # Fall√≥ en tests
                "urls": {
                    "celulares": "https://www.paris.cl/tecnologia/celulares/",
                    "computadores": "https://www.paris.cl/tecnologia/computadores/"
                }
            }
        }
    
    async def get_smart_system(self, retailer: str) -> SmartProxyDataSaver:
        """Obtener sistema inteligente para retailer (con cache)"""
        
        if retailer not in self.systems:
            logger.info(f"Inicializando sistema inteligente para {retailer}")
            
            smart_system = SmartProxyDataSaver(retailer)
            await smart_system.initialize()
            
            self.systems[retailer] = smart_system
            self.global_stats["sessions_created"] += 1
            
            logger.info(f"‚úÖ Sistema {retailer} listo - Ahorro: {smart_system.saver_config['savings_percent']:.1f}%")
        
        return self.systems[retailer]
    
    async def scrape_category(self, retailer: str, category: str, max_products: int = 50) -> Dict[str, Any]:
        """Scrape una categor√≠a espec√≠fica con sistema optimizado"""
        
        start_time = datetime.now()
        
        # Validar retailer soportado
        if retailer not in self.supported_retailers:
            return {"error": f"Retailer {retailer} no soportado"}
        
        retailer_config = self.supported_retailers[retailer]
        
        # Advertir si no es recomendado
        if not retailer_config["recommended"]:
            logger.warning(f"‚ö†Ô∏è {retailer} no es recomendado - {retailer_config['savings_percent']:.1f}% ahorro")
        
        # Obtener URL para categor√≠a
        category_url = retailer_config["urls"].get(category)
        if not category_url:
            return {"error": f"Categor√≠a {category} no soportada para {retailer}"}
        
        logger.info(f"üîç Scraping {retailer}/{category} - URL: {category_url}")
        
        try:
            # Obtener sistema inteligente
            smart_system = await self.get_smart_system(retailer)
            
            # Scrape con fallback autom√°tico
            page, used_proxy, result = await smart_system.smart_scrape_with_fallback(
                category_url, max_retries=3
            )
            
            if not result["success"]:
                return {
                    "retailer": retailer,
                    "category": category,
                    "success": False,
                    "error": result["error"],
                    "attempts": result["attempts"]
                }
            
            # EXTRAER PRODUCTOS (ejemplo b√°sico - adaptar seg√∫n retailer)
            products = await self._extract_products(page, retailer, max_products)
            
            # Obtener estad√≠sticas de la p√°gina  
            # Esto requerir√≠a acceso al route handler, simplificado por ahora
            page_stats = {
                "method_used": result["method"],
                "attempt_number": result["attempt"]
            }
            
            await page.close()
            
            # Calcular m√©tricas
            duration = (datetime.now() - start_time).total_seconds()
            
            # Actualizar estad√≠sticas globales
            self.global_stats["total_requests"] += 1
            
            logger.info(f"‚úÖ {retailer}/{category} - {len(products)} productos - {duration:.1f}s - {result['method']}")
            
            return {
                "retailer": retailer,
                "category": category,
                "success": True,
                "products": products,
                "products_count": len(products),
                "method_used": result["method"],
                "duration_seconds": duration,
                "estimated_savings_percent": retailer_config["savings_percent"],
                "page_stats": page_stats,
                "system_performance": smart_system.get_performance_report()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico en {retailer}/{category}: {e}")
            
            return {
                "retailer": retailer,
                "category": category, 
                "success": False,
                "error": str(e),
                "duration_seconds": (datetime.now() - start_time).total_seconds()
            }
    
    async def _extract_products(self, page, retailer: str, max_products: int) -> List[Dict[str, Any]]:
        """üîß Extraer productos usando selectores funcionales basados en paris_final.py"""
        
        products = []
        logger.info(f"üîç Extrayendo productos de {retailer} (m√©todo paris_final)...")
        
        try:
            if retailer == "paris":
                # üéØ EXTRACTORES FUNCIONALES DE PARIS (basados en paris_final.py)
                product_containers = await page.query_selector_all('div[data-cnstrc-item-id]')
                logger.info(f"üì¶ Paris: Encontrados {len(product_containers)} contenedores con data-cnstrc-item-id")
                
                for i, container in enumerate(product_containers[:max_products]):
                    try:
                        # Extraer informaci√≥n desde data attributes (m√©todo funcional)
                        product_code = await container.get_attribute('data-cnstrc-item-id') or ''
                        product_name = (await container.get_attribute('data-cnstrc-item-name') or '').strip()
                        price_from_data = await container.get_attribute('data-cnstrc-item-price') or ''
                        
                        # Extraer precios usando selectores exactos de paris_final.py
                        current_price_text = ""
                        current_price_elem = await container.query_selector('span.ui-text-\\[13px\\].ui-leading-\\[15px\\].desktop\\:ui-text-lg, span[class*="ui-font-semibold"][class*="desktop:ui-font-medium"]')
                        if current_price_elem:
                            current_price_text = await current_price_elem.inner_text()
                        
                        # Precio anterior (tachado)
                        old_price_text = ""
                        old_price_elem = await container.query_selector('span.ui-line-through.ui-font-semibold')
                        if old_price_elem:
                            old_price_text = await old_price_elem.inner_text()
                        
                        # Construir precio completo para compatibilidad
                        precio_completo = f"{current_price_text}\\n{old_price_text}".strip()
                        
                        if product_code and product_name:
                            products.append({
                                "nombre": product_name,
                                "precio": precio_completo,
                                "retailer": retailer,
                                "extracted_at": datetime.now().isoformat()
                            })
                            logger.debug(f"‚úÖ Paris producto {i+1}: {product_name[:50]}... - {current_price_text}")
                        
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è Error producto Paris {i}: {e}")
                        continue
                        
            elif retailer == "falabella":
                # üéØ EXTRACTORES FUNCIONALES DE FALABELLA (adaptados del portable)
                product_containers = await page.query_selector_all('div[class*="search-results"][class*="grid-pod"], a[data-key]')
                logger.info(f"üì¶ Falabella: Encontrados {len(product_containers)} productos grid-pod")
                
                for i, container in enumerate(product_containers[:max_products]):
                    try:
                        # Buscar nombre con selectores espec√≠ficos
                        name_elem = await container.query_selector('b[class*="copy"], span[class*="copy"], .pod-title, [data-automation*="product-title"]')
                        price_elem = await container.query_selector('[data-internet-price], [data-cmr-price], .price-1, .copy14')
                        
                        name = await name_elem.inner_text() if name_elem else ""
                        price_text = await price_elem.inner_text() if price_elem else ""
                        
                        if name.strip() and name != "N/A":
                            products.append({
                                "nombre": name.strip(),
                                "precio": price_text.strip() if price_text else "No disponible",
                                "retailer": retailer,
                                "extracted_at": datetime.now().isoformat()
                            })
                            logger.debug(f"‚úÖ Falabella producto {i+1}: {name[:50]}... - {price_text}")
                            
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è Error producto Falabella {i}: {e}")
                        continue
                        
            elif retailer == "ripley":
                # üéØ EXTRACTORES FUNCIONALES DE RIPLEY (basados en selectores reales)
                product_containers = await page.query_selector_all('[data-product-id], .catalog-product-item')
                logger.info(f"üì¶ Ripley: Encontrados {len(product_containers)} productos data-product-id")
                
                for i, container in enumerate(product_containers[:max_products]):
                    try:
                        # Usar selectores exactos de ripley funcional
                        name_elem = await container.query_selector('.catalog-product-details__name')
                        price_normal_elem = await container.query_selector('.catalog-prices__list-price.catalog-prices__line_thru')
                        price_offer_elem = await container.query_selector('.catalog-prices__offer-price')
                        price_card_elem = await container.query_selector('.catalog-prices__card-price')
                        
                        name = await name_elem.inner_text() if name_elem else ""
                        
                        # Construir precio completo como en el formato original
                        prices = []
                        if price_normal_elem:
                            normal_text = await price_normal_elem.inner_text()
                            if normal_text:
                                prices.append(f"Normal: {normal_text}")
                        
                        if price_offer_elem:
                            offer_text = await price_offer_elem.inner_text()
                            if offer_text:
                                prices.append(f"Internet: {offer_text}")
                                
                        if price_card_elem:
                            card_text = await price_card_elem.inner_text()
                            if card_text:
                                prices.append(f"Tarjeta: {card_text}")
                        
                        precio_completo = " | ".join(prices) if prices else "No disponible"
                        
                        if name.strip() and name != "N/A":
                            products.append({
                                "nombre": name.strip(),
                                "precio": precio_completo,
                                "retailer": retailer,
                                "extracted_at": datetime.now().isoformat()
                            })
                            logger.debug(f"‚úÖ Ripley producto {i+1}: {name[:50]}... - {precio_completo[:50]}...")
                            
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è Error producto Ripley {i}: {e}")
                        continue
            
            else:
                # üéØ EXTRACTOR GEN√âRICO MEJORADO
                # Buscar m√∫ltiples tipos de contenedores
                selectors = [
                    '[data-product-id]',
                    '[data-cnstrc-item-id]', 
                    'div[class*="product"]',
                    'article[class*="product"]',
                    '.product-item',
                    '.catalog-product-item',
                    'div[class*="grid-pod"]'
                ]
                
                product_containers = []
                for selector in selectors:
                    containers = await page.query_selector_all(selector)
                    if containers:
                        product_containers = containers
                        logger.info(f"üì¶ {retailer}: Usando selector '{selector}' - {len(containers)} productos")
                        break
                
                for i, container in enumerate(product_containers[:max_products]):
                    try:
                        # M√∫ltiples selectores para nombre
                        name_selectors = [
                            '[data-cnstrc-item-name]',
                            '.catalog-product-details__name',
                            'h1, h2, h3',
                            '.title', '.name', '.product-title',
                            'b[class*="copy"]',
                            'span[class*="copy"]'
                        ]
                        
                        name = ""
                        for selector in name_selectors:
                            name_elem = await container.query_selector(selector)
                            if name_elem:
                                name = await name_elem.inner_text()
                                if name.strip():
                                    break
                        
                        # Si es data attribute, intentar obtenerlo
                        if not name:
                            name = await container.get_attribute('data-cnstrc-item-name') or ""
                        
                        # M√∫ltiples selectores para precio
                        price_selectors = [
                            '.catalog-prices__offer-price',
                            '.price', '.prices',
                            '[data-price]', '[data-internet-price]',
                            '.cost', '.amount'
                        ]
                        
                        price_text = ""
                        for selector in price_selectors:
                            price_elem = await container.query_selector(selector)
                            if price_elem:
                                price_text = await price_elem.inner_text()
                                if price_text.strip():
                                    break
                        
                        if name.strip() and name != "N/A":
                            products.append({
                                "nombre": name.strip(),
                                "precio": price_text.strip() if price_text else "No disponible",
                                "retailer": retailer,
                                "extracted_at": datetime.now().isoformat()
                            })
                            logger.debug(f"‚úÖ {retailer} producto {i+1}: {name[:50]}...")
                            
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è Error producto {retailer} {i}: {e}")
                        continue
        
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico en extracci√≥n {retailer}: {e}")
        
        logger.info(f"üéØ {retailer}: Extra√≠dos {len(products)} productos exitosamente")
        return products[:max_products]
    
    async def run_full_scraping_cycle(self, retailers: List[str] = None, max_products_per_category: int = 30) -> Dict[str, Any]:
        """Ejecutar ciclo completo de scraping optimizado"""
        
        if retailers is None:
            # Solo retailers recomendados por defecto
            retailers = [r for r, config in self.supported_retailers.items() if config["recommended"]]
        
        logger.info(f"üöÄ Iniciando ciclo completo - Retailers: {', '.join(retailers)}")
        
        cycle_start = datetime.now()
        results = {
            "cycle_id": f"cycle_{int(cycle_start.timestamp())}",
            "start_time": cycle_start.isoformat(),
            "retailers": retailers,
            "results": {},
            "summary": {
                "total_products": 0,
                "successful_extractions": 0,
                "failed_extractions": 0,
                "total_estimated_savings": 0
            }
        }
        
        for retailer in retailers:
            logger.info(f"\n--- Procesando {retailer.upper()} ---")
            
            retailer_results = {
                "categories": {},
                "total_products": 0,
                "total_duration": 0,
                "success_rate": 0
            }
            
            retailer_config = self.supported_retailers[retailer]
            categories = list(retailer_config["urls"].keys())
            
            successful_extractions = 0
            
            for category in categories:
                logger.info(f"üìÇ Procesando {retailer}/{category}")
                
                category_result = await self.scrape_category(
                    retailer, category, max_products_per_category
                )
                
                retailer_results["categories"][category] = category_result
                retailer_results["total_duration"] += category_result.get("duration_seconds", 0)
                
                if category_result["success"]:
                    successful_extractions += 1
                    retailer_results["total_products"] += category_result["products_count"]
                    results["summary"]["total_products"] += category_result["products_count"]
                
                # Pausa entre categor√≠as
                await asyncio.sleep(2)
            
            # Calcular m√©tricas del retailer
            retailer_results["success_rate"] = (successful_extractions / len(categories)) * 100
            results["results"][retailer] = retailer_results
            
            # Actualizar summary global
            if successful_extractions > 0:
                results["summary"]["successful_extractions"] += successful_extractions
            else:
                results["summary"]["failed_extractions"] += 1
            
            results["summary"]["total_estimated_savings"] += retailer_config["savings_percent"]
            
            logger.info(f"‚úÖ {retailer} completado - {retailer_results['total_products']} productos")
        
        # Finalizar ciclo
        cycle_end = datetime.now()
        results["end_time"] = cycle_end.isoformat()
        results["total_duration_minutes"] = (cycle_end - cycle_start).total_seconds() / 60
        results["summary"]["average_savings_percent"] = results["summary"]["total_estimated_savings"] / len(retailers)
        
        logger.info(f"üéØ Ciclo completado - {results['summary']['total_products']} productos - "
                   f"{results['total_duration_minutes']:.1f} min")
        
        return results
    
    async def save_results(self, results: Dict[str, Any], filename: str = None):
        """Guardar resultados del scraping"""
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"production_results_{timestamp}.json"
        
        output_path = Path("production_results")
        output_path.mkdir(exist_ok=True)
        
        file_path = output_path / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üíæ Resultados guardados: {file_path}")
        
        return file_path
    
    async def cleanup_all(self):
        """Limpiar todos los sistemas"""
        
        for retailer, system in self.systems.items():
            await system.cleanup()
            logger.info(f"üßπ {retailer} limpiado")
        
        self.systems.clear()
        logger.info("‚úÖ Cleanup completo")


async def main():
    """Funci√≥n principal de producci√≥n"""
    
    print("üöÄ SISTEMA DE PRODUCCI√ìN - PROXY INTELIGENTE + AHORRO DE DATOS")
    print("=" * 70)
    
    # Inicializar sistema
    production_system = ProductionScrapingSystem()
    
    try:
        # Ejecutar ciclo completo con retailers recomendados
        results = await production_system.run_full_scraping_cycle(
            retailers=["falabella", "ripley"],  # Solo los que funcionan
            max_products_per_category=20
        )
        
        # Guardar resultados
        results_file = await production_system.save_results(results)
        
        # Mostrar resumen
        print(f"\nüìä RESUMEN DE EJECUCI√ìN:")
        print("-" * 40)
        print(f"Productos extra√≠dos: {results['summary']['total_products']}")
        print(f"Duraci√≥n total: {results['total_duration_minutes']:.1f} minutos") 
        print(f"Ahorro promedio: {results['summary']['average_savings_percent']:.1f}%")
        print(f"Retailers exitosos: {results['summary']['successful_extractions']}")
        print(f"Resultados guardados: {results_file}")
        
        print(f"\nüéØ SISTEMA LISTO PARA PRODUCCI√ìN")
        print("Configuraci√≥n √≥ptima aplicada:")
        print("- ‚úÖ 70% tr√°fico directo / 30% proxy")
        print("- ‚úÖ Fallback autom√°tico en bloqueos")
        print("- ‚úÖ 93.6% ahorro Falabella")
        print("- ‚úÖ 50.1% ahorro Ripley")
        print("- ‚úÖ 33+ dominios bloqueados autom√°ticamente")
        
    except Exception as e:
        logger.error(f"‚ùå Error en ejecuci√≥n principal: {e}")
        print(f"‚ùå Error: {e}")
    
    finally:
        # Cleanup
        await production_system.cleanup_all()
        print(f"\nüßπ Recursos limpiados - Sistema finalizado")


if __name__ == "__main__":
    # Ejecutar sistema de producci√≥n
    asyncio.run(main())